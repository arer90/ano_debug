{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import gc\n",
    "import time\n",
    "import subprocess\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, DataStructs, Draw\n",
    "from rdkit import RDConfig\n",
    "from rdkit.Chem import Descriptors, rdMolDescriptors, Lipinski, rdDistGeom, rdPartialCharges\n",
    "from rdkit.Chem.AllChem import GetMorganGenerator\n",
    "from rdkit.DataStructs.cDataStructs import ConvertToNumpyArray\n",
    "from rdkit.Avalon.pyAvalonTools import GetAvalonFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, root_mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_path = \"result/1_standard_ML\"\n",
    "os.makedirs(target_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ws = pd.read_csv('./data/ws496_logS.csv', dtype={'SMILES': 'string'})\n",
    "smiles_ws = data_ws['SMILES']\n",
    "y_ws = data_ws.iloc[:, 2]\n",
    "\n",
    "data_delaney = pd.read_csv('./data/delaney-processed.csv', dtype={'smiles': 'string'})\n",
    "smiles_de = data_delaney['smiles']\n",
    "y_de = data_delaney.iloc[:, 1]\n",
    "\n",
    "data_lovric2020 = pd.read_csv('./data/Lovric2020_logS0.csv', dtype={'isomeric_smiles': 'string'})\n",
    "smiles_lo = data_lovric2020['isomeric_smiles']\n",
    "y_lo = data_lovric2020.iloc[:, 1]\n",
    "\n",
    "data_huuskonen = pd.read_csv('./data/huusk.csv', dtype={'SMILES': 'string'})\n",
    "smiles_hu = data_huuskonen['SMILES']\n",
    "y_hu = data_huuskonen.iloc[:, -1].astype('float')\n",
    "\n",
    "data_bigsoldb = pd.read_csv('./data/BigSolDB.csv', dtype={'SMILES':'string'})\n",
    "smiles_bd = data_bigsoldb['SMILES']\n",
    "y_bd = data_bigsoldb.iloc[:, 2].astype('float')\n",
    "\n",
    "data_aqsoldb = pd.read_csv('./data/curated-solubility-dataset.csv', dtype={'SMILES':'string'})\n",
    "smiles_aq = data_aqsoldb['SMILES']\n",
    "y_aq = data_aqsoldb.iloc[:, 5].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_advanced_histograms(data_frames, save_path=\"result/dataset_plot\"):\n",
    "    save_path = os.path.join(save_path, 'histogram')\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    for df_name, (smiles, y_values) in data_frames.items():\n",
    "        print(f\"Processing dataset: {df_name}\")\n",
    "        \n",
    "        outliers_txt_path = os.path.join(save_path, f\"{df_name}_outliers_info.txt\")  # Fixed path\n",
    "        \n",
    "        with open(outliers_txt_path, 'w') as outlier_file:\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            \n",
    "            # Histogram with KDE plot\n",
    "            sns.histplot(y_values, bins=30, kde=True, ax=ax, color='blue', alpha=0.6, edgecolor='black')  # Fixed bins\n",
    "            \n",
    "            # IQR for outlier detection\n",
    "            Q1 = np.percentile(y_values, 25)\n",
    "            Q3 = np.percentile(y_values, 75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            outliers = y_values[(y_values < lower_bound) | (y_values > upper_bound)]\n",
    "            \n",
    "            outlier_indices = np.where((y_values < lower_bound) | (y_values > upper_bound))[0]\n",
    "            if len(outlier_indices) > 0:\n",
    "                outlier_file.write(f\"Outliers for {df_name}:\\n\")\n",
    "                for idx in outlier_indices:\n",
    "                    # Adjusted to handle indexing in case smiles is not a Series\n",
    "                    outlier_info = f\"Index: {idx}, SMILES: {smiles[idx]}, Value: {y_values[idx]}\\n\"  # Fixed indexing\n",
    "                    outlier_file.write(outlier_info)\n",
    "            \n",
    "            # Mark outliers on the plot\n",
    "            ax.scatter(outliers, np.zeros_like(outliers), color='red', s=10, label=f'Outliers: {len(outliers)}')\n",
    "            \n",
    "            # Mean, median, standard deviation\n",
    "            mean_val = np.mean(y_values)\n",
    "            median_val = np.median(y_values)\n",
    "            std_val = np.std(y_values)\n",
    "            \n",
    "            ax.axvline(mean_val, color='r', linestyle='--', linewidth=1.5, label=f'Mean: {mean_val:.2f}')\n",
    "            ax.axvline(median_val, color='g', linestyle='-', linewidth=1.5, label=f'Median: {median_val:.2f}')\n",
    "            ax.fill_betweenx([0, ax.get_ylim()[1]], mean_val - std_val, mean_val + std_val, color='r', alpha=0.1, label=f'STD: ±{std_val:.2f}')\n",
    "            \n",
    "            # Dynamic X range adjustment based on IQR\n",
    "            ax.set_xlim(lower_bound - IQR, upper_bound + IQR)\n",
    "            \n",
    "            ax.set_title(f\"Histogram and KDE with Outliers for {df_name}\")\n",
    "            ax.set_xlabel('LogS')\n",
    "            ax.set_ylabel('Frequency')\n",
    "            ax.legend()\n",
    "            ax.grid(axis='y', alpha=0.75)\n",
    "            \n",
    "            # Save plot\n",
    "            fig_save_path = os.path.join(save_path, f\"{df_name}_histogram_kde.png\")  # Fixed path\n",
    "            plt.savefig(fig_save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"File saved to: {fig_save_path}\")\n",
    "            \n",
    "            # Show plot\n",
    "            plt.show()\n",
    "            plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frames = {\n",
    "    'ws496': (smiles_ws, y_ws),\n",
    "    'delaney': (smiles_de, y_de),\n",
    "    'lovric': (smiles_lo, y_lo),\n",
    "    'huusk': (smiles_hu, y_hu),\n",
    "    'AqSolDb':(smiles_aq, y_aq),\n",
    "    'BigSolDB':(smiles_bd,y_bd),\n",
    "}\n",
    "plot_advanced_histograms(data_frames, target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    'ws496': y_ws,\n",
    "    'delaney': y_de,\n",
    "    'lovric': y_lo,\n",
    "    'huusk': y_hu\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_combined_histogram(data, save_path=\"result/dataset_plot\"):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    if isinstance(data, dict):\n",
    "        data = pd.DataFrame(data)\n",
    "    \n",
    "    dataset_names = data.columns\n",
    "    num_datasets = len(dataset_names)\n",
    "    \n",
    "    all_data = np.concatenate([data[col].dropna().values for col in dataset_names])\n",
    "    bins = np.linspace(min(all_data), max(all_data), 31)\n",
    "    \n",
    "    counts_list = [np.histogram(data[col].dropna(), bins=bins)[0] for col in dataset_names]\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "    width = (bins[1] - bins[0]) * 0.7\n",
    "\n",
    "    for i, col in enumerate(dataset_names):\n",
    "        fig.add_trace(go.Bar(x=bin_centers, y=counts_list[i], width=width, name=col))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        barmode='group',\n",
    "        xaxis_title='LogS',\n",
    "        yaxis_title='Frequency',\n",
    "        title='Combined Histogram of LogS Values',\n",
    "        legend=dict(x=0.1, y=0.9, bgcolor='rgba(255,255,255,0.5)'),\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    \n",
    "    # HTML 저장\n",
    "    html_save_file_path = f\"{save_path}/LogS_Frequency_plotly.html\"\n",
    "    fig.write_html(html_save_file_path)\n",
    "    print(f\"HTML file saved to: {html_save_file_path}\")\n",
    "    \n",
    "    fig.show()\n",
    "plot_combined_histogram(data_dict, target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_combined_histogram(data, save_path=\"result/dataset_plot\"):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    if isinstance(data, dict):\n",
    "        data = pd.DataFrame(data)\n",
    "    \n",
    "    dataset_names = data.columns\n",
    "    all_data = np.concatenate([data[col].dropna().values for col in dataset_names])\n",
    "    bins = np.linspace(min(all_data), max(all_data), 31)\n",
    "\n",
    "    plt.figure(figsize=(14, 8))\n",
    "\n",
    "    for col in dataset_names:\n",
    "        sns.histplot(data[col].dropna(), bins=bins, label=col, kde=False, edgecolor='black')\n",
    "\n",
    "    plt.legend(loc='upper right', fontsize=12)\n",
    "    plt.xlabel('LogS', fontsize=16, weight='bold')\n",
    "    plt.ylabel('Frequency', fontsize=16, weight='bold')\n",
    "    plt.title('Combined Histogram of LogS Values', fontsize=18, weight='bold')\n",
    "\n",
    "    # Save as PNG\n",
    "    png_save_file_path = f\"{save_path}/LogS_Frequency_seaborn.png\"\n",
    "    plt.savefig(png_save_file_path, dpi=300)\n",
    "    print(f\"PNG file saved to: {png_save_file_path}\")\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n",
    "\n",
    "plot_combined_histogram(data_dict, target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data_dict).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([y_ws, y_de,y_lo,y_hu], label=['ws496','delaney','lovric','huusk'])\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('LogS')\n",
    "plt.ylabel('Frequency')\n",
    "plt.savefig(f\"{target_path}/1_LogS_Frequency.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([y_ws, y_de,y_lo,y_hu,y_aq], label=['ws496','delaney','lovric','huusk','AqSolDB'])\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('LogS')\n",
    "plt.ylabel('Frequency')\n",
    "plt.savefig(f\"{target_path}/1_LogS_Frequency_with_aq.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([y_ws, y_de,y_lo,y_hu,y_aq,y_bd], label=['ws496','delaney','lovric','huusk','AqSolDB','BigSolDb'])\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('LogS')\n",
    "plt.ylabel('Frequency')\n",
    "plt.savefig(f\"{target_path}/1_LogS_Frequency_with_all.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mol3d(mol):\n",
    "    mol = Chem.AddHs(mol)\n",
    "    optimization_methods = [\n",
    "        (AllChem.EmbedMolecule, (mol, AllChem.ETKDGv3()), {}),\n",
    "        (AllChem.UFFOptimizeMolecule, (mol,), {'maxIters': 200}),\n",
    "        (AllChem.MMFFOptimizeMolecule, (mol,), {'maxIters': 200})\n",
    "    ]\n",
    "\n",
    "    for method, args, kwargs in optimization_methods:\n",
    "        try:\n",
    "            method(*args, **kwargs)\n",
    "            if mol.GetNumConformers() > 0:\n",
    "                return mol\n",
    "        except ValueError as e:\n",
    "            print(f\"Error: {e} - Trying next optimization method [{method}]\")\n",
    "\n",
    "    print(f\"Invalid mol for 3d {'\\033[94m'}{Chem.MolToSmiles(mol)}{'\\033[0m'} - No conformer generated\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_smiles_to_mol(smiles, fail_folder=None, index=None, yvalue=None):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        print(f\"[convert_smiles_to_mol] Cannot convert {smiles} to Mols\")\n",
    "        return None, {\"smiles\": smiles, \"y_value\": yvalue, \"error\": \"Invalid SMILES\"}\n",
    "\n",
    "    try:\n",
    "        Chem.Kekulize(mol, clearAromaticFlags=True)\n",
    "        isomeric_smiles = Chem.MolToSmiles(mol, isomericSmiles=True)\n",
    "        mol = Chem.MolFromSmiles(isomeric_smiles)\n",
    "    except Exception as e:\n",
    "        print(f\"[convert_smiles_to_mol] failed {smiles} isomeric_smiles by {e}\")\n",
    "        if fail_folder and index is not None:\n",
    "            img_path = os.path.join(fail_folder, f\"mol_{index}.png\")\n",
    "            img = Draw.MolToImage(mol)\n",
    "            img.save(img_path)\n",
    "        return None, {\"smiles\": smiles, \"y_value\": yvalue, \"error\": f\"Isomeric SMILES error: {e}\"}\n",
    "\n",
    "    try:\n",
    "        Chem.SanitizeMol(mol)\n",
    "    except Exception as e:\n",
    "        print(f\"[convert_smiles_to_mol] failed {smiles} SanitizeMol by {e}\")\n",
    "        if fail_folder and index is not None:\n",
    "            img_path = os.path.join(fail_folder, f\"mol_{index}.png\")\n",
    "            img = Draw.MolToImage(mol)\n",
    "            img.save(img_path)\n",
    "        return None, {\"smiles\": smiles, \"y_value\": yvalue, \"error\": f\"SanitizeMol error: {e}\"}\n",
    "\n",
    "    return mol, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_smiles(smiles, yvalue, fail_folder, index):\n",
    "    mol, error = convert_smiles_to_mol(smiles, fail_folder, index, yvalue)\n",
    "    if error:\n",
    "        return None, None, error\n",
    "\n",
    "    mol_3d = mol3d(mol)\n",
    "    if mol_3d:\n",
    "        return smiles, yvalue, None\n",
    "    else:\n",
    "        img_path = os.path.join(fail_folder, f\"mol_{index}.png\")\n",
    "        img = Draw.MolToImage(mol)\n",
    "        img.save(img_path)\n",
    "        return None, None, {\"smiles\": smiles, \"y_value\": yvalue}\n",
    "\n",
    "def process_dataset(smiles_list, y_values, dataset_name, target_path=\"result\", max_workers=None):\n",
    "    start = time.time()\n",
    "    valid_smiles, valid_y = [], []\n",
    "    error_smiles_list = []\n",
    "    fail_folder = f\"{target_path}/failed/{dataset_name}\"\n",
    "    os.makedirs(fail_folder, exist_ok=True)\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [\n",
    "            executor.submit(process_smiles, smiles, yvalue, fail_folder, i)\n",
    "            for i, (smiles, yvalue) in enumerate(zip(smiles_list, y_values))\n",
    "        ]\n",
    "        for future in as_completed(futures):\n",
    "            smiles, yvalue, error = future.result()\n",
    "            if error:\n",
    "                error_smiles_list.append(error)\n",
    "            elif smiles is not None and yvalue is not None:\n",
    "                valid_smiles.append(smiles)\n",
    "                valid_y.append(yvalue)\n",
    "\n",
    "    if error_smiles_list:\n",
    "        error_df = pd.DataFrame(error_smiles_list)\n",
    "        error_df.to_csv(os.path.join(fail_folder, \"failed_smiles.csv\"), index=False)\n",
    "    print(f\" [{dataset_name:<10}] : {time.time()-start:.4f} sec\")\n",
    "    return valid_smiles, valid_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_ws, y_ws = process_dataset(smiles_ws, y_ws, \"ws496\", target_path)\n",
    "smiles_de, y_de = process_dataset(smiles_de, y_de, \"delaney\", target_path)\n",
    "smiles_lo, y_lo = process_dataset(smiles_lo, y_lo, \"Lovric2020_logS0\", target_path)\n",
    "smiles_hu, y_hu = process_dataset(smiles_hu, y_hu, \"huusk\", target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_aq, y_aq = process_dataset(smiles_aq, y_aq, \"AqSolDB\", target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_bd, y_bd = process_dataset(smiles_bd, y_bd, \"BigSolDB\", target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEN_OF_FF = 2048\n",
    "LEN_OF_MA = 167\n",
    "LEN_OF_AV = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fingerprints(mol):\n",
    "    if mol is None:\n",
    "        return None, None, None\n",
    "    \n",
    "    morgan_generator = GetMorganGenerator(radius=2, fpSize=LEN_OF_FF)\n",
    "    ecfp = morgan_generator.GetFingerprint(mol)\n",
    "    ecfp_array = np.zeros((LEN_OF_FF,),dtype=int)\n",
    "    DataStructs.ConvertToNumpyArray(ecfp, ecfp_array)\n",
    "    \n",
    "    maccs = Chem.rdMolDescriptors.GetMACCSKeysFingerprint(mol)\n",
    "\n",
    "    avalon_fp = GetAvalonFP(mol)\n",
    "    avalon_array = np.zeros((LEN_OF_AV,),dtype=int)\n",
    "    DataStructs.ConvertToNumpyArray(avalon_fp, avalon_array)\n",
    "    \n",
    "    return ecfp_array, maccs, avalon_array\n",
    "\n",
    "def fp_converter(data, use_parallel=True):\n",
    "    mols = [Chem.MolFromSmiles(smi) for smi in data]\n",
    "    \n",
    "    if use_parallel:\n",
    "        try:            \n",
    "            with ProcessPoolExecutor() as executor:\n",
    "                results = list(executor.map(get_fingerprints, mols))\n",
    "        except Exception as e:\n",
    "            print(f\"Parallel processing failed due to: {e}. Falling back to sequential processing.\")\n",
    "            use_parallel = False\n",
    "    \n",
    "    if not use_parallel:\n",
    "        results = [get_fingerprints(mol) for mol in mols]\n",
    "    \n",
    "    ECFP, MACCS, AvalonFP = zip(*results)\n",
    "    \n",
    "    ECFP_container = np.vstack([arr for arr in ECFP if arr is not None])\n",
    "    MACCS_container = np.zeros((len(MACCS), LEN_OF_MA), dtype=int)\n",
    "    AvalonFP_container = np.vstack([arr for arr in AvalonFP if arr is not None])\n",
    "\n",
    "    for i, fp in enumerate(MACCS):\n",
    "        if fp is not None:\n",
    "            DataStructs.ConvertToNumpyArray(fp, MACCS_container[i])\n",
    "    \n",
    "    return mols, ECFP_container, MACCS_container, AvalonFP_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mol_ws, x_ws, MACCS_ws, AvalonFP_ws = fp_converter(smiles_ws)\n",
    "mol_de, x_de, MACCS_de, AvalonFP_de = fp_converter(smiles_de)\n",
    "mol_lo, x_lo, MACCS_lo, AvalonFP_lo = fp_converter(smiles_lo)\n",
    "mol_hu, x_hu, MACCS_hu, AvalonFP_hu = fp_converter(smiles_hu)\n",
    "mol_aq, x_aq, MACCS_aq, AvalonFP_aq = fp_converter(smiles_aq)\n",
    "mol_bd, x_bd, MACCS_bd, AvalonFP_bd = fp_converter(smiles_bd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtr_ws, xte_ws, ytr_ws, yte_ws = train_test_split(x_ws, y_ws, test_size=0.2,random_state=42)\n",
    "xtr_de, xte_de, ytr_de, yte_de = train_test_split(x_de, y_de, test_size=0.2,random_state=42)\n",
    "xtr_lo, xte_lo, ytr_lo, yte_lo = train_test_split(x_lo, y_lo, test_size=0.2,random_state=42)\n",
    "xtr_hu, xte_hu, ytr_hu, yte_hu = train_test_split(x_hu, y_hu, test_size=0.2,random_state=42)\n",
    "xtr_aq, xte_aq, ytr_aq, yte_aq = train_test_split(x_aq, y_aq, test_size=0.2,random_state=42)\n",
    "xtr_bd, xte_bd, ytr_bd, yte_bd = train_test_split(x_bd, y_bd, test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(\n",
    "            units=1024,\n",
    "            activation='relu',\n",
    "            kernel_initializer='glorot_uniform',\n",
    "            kernel_regularizer=regularizers.l2(1e-4)),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(\n",
    "            units=496,\n",
    "            kernel_initializer='glorot_uniform',\n",
    "            kernel_regularizer=regularizers.l2(1e-4),\n",
    "            activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(units=1, dtype='float32')\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), \n",
    "                  loss=tf.keras.losses.MeanSquaredError(),\n",
    "                  metrics=[tf.keras.metrics.MeanSquaredError(), \n",
    "                           tf.keras.metrics.MeanAbsoluteError(),\n",
    "                           tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model\n",
    "\n",
    "def multi_models(xtr, ytr):\n",
    "    ridge = Ridge().fit(xtr, ytr)\n",
    "    mlp = MLPRegressor(random_state=42, max_iter=350).fit(xtr, ytr)\n",
    "    rfr = RandomForestRegressor(random_state=42, n_estimators=100).fit(xtr, ytr)\n",
    "    svr = SVR().fit(xtr, ytr)\n",
    "    tmp = new_model()\n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((xtr, ytr))\n",
    "    train_dataset = train_dataset.batch(32).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    tmp.fit(train_dataset, epochs=100, verbose=0)\n",
    "    group = [ridge, mlp, rfr, svr, tmp]\n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws_models = multi_models(xtr_ws, ytr_ws)\n",
    "de_models = multi_models(xtr_de, ytr_de)\n",
    "lo_models = multi_models(xtr_lo, ytr_lo)\n",
    "hu_models = multi_models(xtr_hu, ytr_hu)\n",
    "aq_models = multi_models(xtr_aq, ytr_aq)\n",
    "bd_models = multi_models(xtr_bd, ytr_bd)\n",
    "# 1M 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(dataset, model_name, ypre,yte):\n",
    "    r2 = r2_score(yte,ypre)\n",
    "    mae  = mean_absolute_error(yte,ypre)\n",
    "    mse  = mean_squared_error(yte,ypre)\n",
    "    rmse = root_mean_squared_error(yte,ypre)\n",
    "    print(f\"[data : {dataset} ][model : {model_name} ] = r2 : {r2:.5f}, mae : {mae:.5f}, mse : {mse:.5f}, rmse : {rmse:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_res(pred, truth):\n",
    "    flat = pred[4].flatten()\n",
    "    res = pd.DataFrame({'ytruth':truth,\n",
    "                        'RR ':pred[0],\n",
    "                        'MLP':pred[1],\n",
    "                        'RFR':pred[2],\n",
    "                        'SVR':pred[3],\n",
    "                        'DNN':flat,\n",
    "                        }) #,   \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics('ws','RID', ws_models[0].predict(xte_ws), yte_ws)\n",
    "print_metrics('ws','MLP', ws_models[1].predict(xte_ws), yte_ws)\n",
    "print_metrics('ws','RFR', ws_models[2].predict(xte_ws), yte_ws)\n",
    "print_metrics('ws','SVR', ws_models[3].predict(xte_ws), yte_ws)\n",
    "print_metrics('ws','DNN', ws_models[4].predict(xte_ws, verbose=0), yte_ws)\n",
    "res_rid = ws_models[0].predict(xte_ws)\n",
    "res_mlp = ws_models[1].predict(xte_ws)\n",
    "res_rfr = ws_models[2].predict(xte_ws)\n",
    "res_svr = ws_models[3].predict(xte_ws)\n",
    "res_dnn = ws_models[4].predict(xte_ws, verbose=0)\n",
    "ws_res_all = save_res([res_rid, res_mlp, res_rfr, res_svr, res_dnn], yte_ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics('de','RID', de_models[0].predict(xte_de), yte_de)\n",
    "print_metrics('de','MLP', de_models[1].predict(xte_de), yte_de)\n",
    "print_metrics('de','RFR', de_models[2].predict(xte_de), yte_de)\n",
    "print_metrics('de','SVR', de_models[3].predict(xte_de), yte_de)\n",
    "print_metrics('de','DNN', de_models[4].predict(xte_de, verbose=0), yte_de)\n",
    "res_rid = de_models[0].predict(xte_de)\n",
    "res_mlp = de_models[1].predict(xte_de)\n",
    "res_rfr = de_models[2].predict(xte_de)\n",
    "res_svr = de_models[3].predict(xte_de)\n",
    "res_dnn = de_models[4].predict(xte_de, verbose=0)\n",
    "de_res_all = save_res([res_rid, res_mlp, res_rfr, res_svr, res_dnn], yte_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics('lo','RID', lo_models[0].predict(xte_lo), yte_lo)\n",
    "print_metrics('lo','MLP', lo_models[1].predict(xte_lo), yte_lo)\n",
    "print_metrics('lo','RFR', lo_models[2].predict(xte_lo), yte_lo)\n",
    "print_metrics('lo','SVR', lo_models[3].predict(xte_lo), yte_lo)\n",
    "print_metrics('lo','DNN', lo_models[4].predict(xte_lo, verbose=0), yte_lo)\n",
    "res_rid = lo_models[0].predict(xte_lo)\n",
    "res_mlp = lo_models[1].predict(xte_lo)\n",
    "res_rfr = lo_models[2].predict(xte_lo)\n",
    "res_svr = lo_models[3].predict(xte_lo)\n",
    "res_dnn = lo_models[4].predict(xte_lo, verbose=0)\n",
    "lo_res_all = save_res([res_rid, res_mlp, res_rfr, res_svr, res_dnn], yte_lo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics('hu','RID', hu_models[0].predict(xte_hu), yte_hu)\n",
    "print_metrics('hu','MLP', hu_models[1].predict(xte_hu), yte_hu)\n",
    "print_metrics('hu','RFR', hu_models[2].predict(xte_hu), yte_hu)\n",
    "print_metrics('hu','SVR', hu_models[3].predict(xte_hu), yte_hu)\n",
    "print_metrics('hu','DNN', hu_models[4].predict(xte_hu, verbose=0), yte_hu)\n",
    "res_rid = hu_models[0].predict(xte_hu)\n",
    "res_mlp = hu_models[1].predict(xte_hu)\n",
    "res_rfr = hu_models[2].predict(xte_hu)\n",
    "res_svr = hu_models[3].predict(xte_hu)\n",
    "res_dnn = hu_models[4].predict(xte_hu, verbose=0)\n",
    "hu_res_all = save_res([res_rid, res_mlp, res_rfr, res_svr, res_dnn], yte_hu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics('aq','RID', aq_models[0].predict(xte_aq), yte_aq)\n",
    "print_metrics('aq','MLP', aq_models[1].predict(xte_aq), yte_aq)\n",
    "print_metrics('aq','RFR', aq_models[2].predict(xte_aq), yte_aq)\n",
    "print_metrics('aq','SVR', aq_models[3].predict(xte_aq), yte_aq)\n",
    "print_metrics('aq','DNN', aq_models[4].predict(xte_aq, verbose=0), yte_aq)\n",
    "res_rid = aq_models[0].predict(xte_aq)\n",
    "res_mlp = aq_models[1].predict(xte_aq)\n",
    "res_rfr = aq_models[2].predict(xte_aq)\n",
    "res_svr = aq_models[3].predict(xte_aq)\n",
    "res_dnn = aq_models[4].predict(xte_aq, verbose=0)\n",
    "aq_res_all = save_res([res_rid, res_mlp, res_rfr, res_svr, res_dnn], yte_aq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics('bd','RID', bd_models[0].predict(xte_bd), yte_bd)\n",
    "print_metrics('bd','MLP', bd_models[1].predict(xte_bd), yte_bd)\n",
    "print_metrics('bd','RFR', bd_models[2].predict(xte_bd), yte_bd)\n",
    "print_metrics('bd','SVR', bd_models[3].predict(xte_bd), yte_bd)\n",
    "print_metrics('bd','DNN', bd_models[4].predict(xte_bd, verbose=0), yte_bd)\n",
    "res_rid = bd_models[0].predict(xte_bd)\n",
    "res_mlp = bd_models[1].predict(xte_bd)\n",
    "res_rfr = bd_models[2].predict(xte_bd)\n",
    "res_svr = bd_models[3].predict(xte_bd)\n",
    "res_dnn = bd_models[4].predict(xte_bd, verbose=0)\n",
    "bd_res_all = save_res([res_rid, res_mlp, res_rfr, res_svr, res_dnn], yte_bd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupA = ['ws496', ws_res_all]\n",
    "groupB = ['delaney', de_res_all]\n",
    "groupC = ['lovrics', lo_res_all]\n",
    "groupD = ['Huusken', hu_res_all]\n",
    "groupE = ['AqSolDB', aq_res_all]\n",
    "groupF = ['BigSolDB', bd_res_all]\n",
    "res_datasets = [groupA, groupB, groupC, groupD, groupE, groupF]\n",
    "res_datasets_ori = [groupA, groupB, groupC, groupD]\n",
    "res_datasets_aq = [groupA, groupB, groupC, groupD, groupE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_res(res_datasets, name, path, b=25, w=6):\n",
    "    fig = plt.figure(constrained_layout=True, figsize=(b, w))\n",
    "    fig.suptitle(\"Standard Model Prediction\", fontsize=20)\n",
    "\n",
    "    available_markers = ['o', 's', 'D', 'P', 'X', '^', 'v', '<', '>', 'h', '8', 'p']\n",
    "    color_cycle = plt.get_cmap('rainbow')  \n",
    "    marker_size = 20\n",
    "\n",
    "    for i, k in enumerate(res_datasets):\n",
    "        ax = fig.add_subplot(1, len(res_datasets), i+1)\n",
    "        ax.set_title(k[0], fontsize=20)\n",
    "\n",
    "        y_truth = k[1].iloc[:, 0]\n",
    "\n",
    "        ax.plot(y_truth, y_truth, 'k-', label=None)\n",
    "\n",
    "        for j in range(1, len(k[1].columns)):\n",
    "            color = color_cycle(j / (len(k[1].columns) - 1))\n",
    "            marker = available_markers[(j-1) % len(available_markers)]\n",
    "\n",
    "            model_name = f\"{k[1].columns.values[j]:<3}\"\n",
    "            r2_value = f\"{r2_score(y_truth, k[1].iloc[:, j]):.3f}\"\n",
    "            label_text = f\"{model_name}: R2 = {r2_value} %\"\n",
    "\n",
    "            ax.scatter(y_truth, k[1].iloc[:, j], marker=marker, color=color, s=marker_size,\n",
    "                       label=label_text)\n",
    "\n",
    "        ax.legend(loc='lower right', prop={'size': 13})\n",
    "        ax.set_ylabel('Y Truth', fontsize=15)\n",
    "        ax.set_xlabel('Y Prediction', fontsize=15)\n",
    "\n",
    "    if name == \"classic\":\n",
    "        plt.savefig(f\"{path}/1_standard_model_compare.png\", dpi=300)\n",
    "    else:\n",
    "        plt.savefig(f\"{path}/1_standard_model_compare_{name}.png\", dpi=300)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_res(res_datasets_ori, 'classic',target_path, b=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_res(res_datasets_aq, 'AqSol',target_path, b=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_res(res_datasets, 'Bigdata',target_path, b=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
