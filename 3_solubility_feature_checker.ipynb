{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import gc\n",
    "import time\n",
    "import subprocess\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, DataStructs, Draw\n",
    "from rdkit import RDConfig\n",
    "from rdkit.Chem import Descriptors, rdMolDescriptors, Lipinski, rdDistGeom, rdPartialCharges\n",
    "from rdkit.Chem.AllChem import GetMorganGenerator\n",
    "from rdkit.DataStructs.cDataStructs import ConvertToNumpyArray\n",
    "from rdkit.Avalon.pyAvalonTools import GetAvalonFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, root_mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_path = \"result/3_solubility_feature_checker\"\n",
    "os.makedirs(target_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ws = pd.read_csv('./data/ws496_logS.csv', dtype={'SMILES': 'string'})\n",
    "smiles_ws = data_ws['SMILES']\n",
    "y_ws = data_ws.iloc[:, 2]\n",
    "\n",
    "data_delaney = pd.read_csv('./data/delaney-processed.csv', dtype={'smiles': 'string'})\n",
    "smiles_de = data_delaney['smiles']\n",
    "y_de = data_delaney.iloc[:, 1]\n",
    "\n",
    "data_lovric2020 = pd.read_csv('./data/Lovric2020_logS0.csv', dtype={'isomeric_smiles': 'string'})\n",
    "smiles_lo = data_lovric2020['isomeric_smiles']\n",
    "y_lo = data_lovric2020.iloc[:, 1]\n",
    "\n",
    "data_huuskonen = pd.read_csv('./data/huusk.csv', dtype={'SMILES': 'string'})\n",
    "smiles_hu = data_huuskonen['SMILES']\n",
    "y_hu = data_huuskonen.iloc[:, -1].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mol3d(mol):\n",
    "    mol = Chem.AddHs(mol)\n",
    "    optimization_methods = [\n",
    "        (AllChem.EmbedMolecule, (mol, AllChem.ETKDGv3()), {}),\n",
    "        (AllChem.UFFOptimizeMolecule, (mol,), {'maxIters': 200}),\n",
    "        (AllChem.MMFFOptimizeMolecule, (mol,), {'maxIters': 200})\n",
    "    ]\n",
    "\n",
    "    for method, args, kwargs in optimization_methods:\n",
    "        try:\n",
    "            method(*args, **kwargs)\n",
    "            if mol.GetNumConformers() > 0:\n",
    "                return mol\n",
    "        except ValueError as e:\n",
    "            print(f\"Error: {e} - Trying next optimization method [{method}]\")\n",
    "\n",
    "    print(f\"Invalid mol for 3d {'\\033[94m'}{Chem.MolToSmiles(mol)}{'\\033[0m'} - No conformer generated\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_smiles_to_mol(smiles, fail_folder=None, index=None, yvalue=None):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        print(f\"[convert_smiles_to_mol] Cannot convert {smiles} to Mols\")\n",
    "        return None, {\"smiles\": smiles, \"y_value\": yvalue, \"error\": \"Invalid SMILES\"}\n",
    "\n",
    "    try:\n",
    "        Chem.Kekulize(mol, clearAromaticFlags=True)\n",
    "        isomeric_smiles = Chem.MolToSmiles(mol, isomericSmiles=True)\n",
    "        mol = Chem.MolFromSmiles(isomeric_smiles)\n",
    "    except Exception as e:\n",
    "        print(f\"[convert_smiles_to_mol] failed {smiles} isomeric_smiles by {e}\")\n",
    "        if fail_folder and index is not None:\n",
    "            img_path = os.path.join(fail_folder, f\"mol_{index}.png\")\n",
    "            img = Draw.MolToImage(mol)\n",
    "            img.save(img_path)\n",
    "        return None, {\"smiles\": smiles, \"y_value\": yvalue, \"error\": f\"Isomeric SMILES error: {e}\"}\n",
    "\n",
    "    try:\n",
    "        Chem.SanitizeMol(mol)\n",
    "    except Exception as e:\n",
    "        print(f\"[convert_smiles_to_mol] failed {smiles} SanitizeMol by {e}\")\n",
    "        if fail_folder and index is not None:\n",
    "            img_path = os.path.join(fail_folder, f\"mol_{index}.png\")\n",
    "            img = Draw.MolToImage(mol)\n",
    "            img.save(img_path)\n",
    "        return None, {\"smiles\": smiles, \"y_value\": yvalue, \"error\": f\"SanitizeMol error: {e}\"}\n",
    "\n",
    "    return mol, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_smiles(smiles, yvalue, fail_folder, index):\n",
    "    mol, error = convert_smiles_to_mol(smiles, fail_folder, index, yvalue)\n",
    "    if error:\n",
    "        return None, None, error\n",
    "\n",
    "    mol_3d = mol3d(mol)\n",
    "    if mol_3d:\n",
    "        return smiles, yvalue, None\n",
    "    else:\n",
    "        img_path = os.path.join(fail_folder, f\"mol_{index}.png\")\n",
    "        img = Draw.MolToImage(mol)\n",
    "        img.save(img_path)\n",
    "        return None, None, {\"smiles\": smiles, \"y_value\": yvalue}\n",
    "\n",
    "def process_dataset(smiles_list, y_values, dataset_name, target_path=\"result\", max_workers=None):\n",
    "    start = time.time()\n",
    "    valid_smiles, valid_y = [], []\n",
    "    error_smiles_list = []\n",
    "    fail_folder = f\"{target_path}/failed/{dataset_name}\"\n",
    "    os.makedirs(fail_folder, exist_ok=True)\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [\n",
    "            executor.submit(process_smiles, smiles, yvalue, fail_folder, i)\n",
    "            for i, (smiles, yvalue) in enumerate(zip(smiles_list, y_values))\n",
    "        ]\n",
    "        for future in as_completed(futures):\n",
    "            smiles, yvalue, error = future.result()\n",
    "            if error:\n",
    "                error_smiles_list.append(error)\n",
    "            elif smiles is not None and yvalue is not None:\n",
    "                valid_smiles.append(smiles)\n",
    "                valid_y.append(yvalue)\n",
    "\n",
    "    if error_smiles_list:\n",
    "        error_df = pd.DataFrame(error_smiles_list)\n",
    "        error_df.to_csv(os.path.join(fail_folder, \"failed_smiles.csv\"), index=False)\n",
    "    print(f\" [{dataset_name:<10}] : {time.time()-start:.4f} sec\")\n",
    "    return valid_smiles, valid_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smiles_ws, y_ws = process_dataset(smiles_ws, y_ws, \"ws496\", target_path)\n",
    "smiles_de, y_de = process_dataset(smiles_de, y_de, \"delaney\", target_path)\n",
    "# smiles_lo, y_lo = process_dataset(smiles_lo, y_lo, \"Lovric2020_logS0\", target_path)\n",
    "# smiles_hu, y_hu = process_dataset(smiles_hu, y_hu, \"huusk\", target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEN_OF_FF = 2048\n",
    "LEN_OF_MA = 167\n",
    "LEN_OF_AV = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fingerprints(mol):\n",
    "    if mol is None:\n",
    "        return None, None, None\n",
    "    \n",
    "    morgan_generator = GetMorganGenerator(radius=2, fpSize=LEN_OF_FF)\n",
    "    ecfp = morgan_generator.GetFingerprint(mol)\n",
    "    ecfp_array = np.zeros((LEN_OF_FF,),dtype=int)\n",
    "    DataStructs.ConvertToNumpyArray(ecfp, ecfp_array)\n",
    "    \n",
    "    maccs = Chem.rdMolDescriptors.GetMACCSKeysFingerprint(mol)\n",
    "\n",
    "    avalon_fp = GetAvalonFP(mol)\n",
    "    avalon_array = np.zeros((LEN_OF_AV,),dtype=int)\n",
    "    DataStructs.ConvertToNumpyArray(avalon_fp, avalon_array)\n",
    "    \n",
    "    return ecfp_array, maccs, avalon_array\n",
    "\n",
    "def fp_converter(data, use_parallel=True):\n",
    "    mols = [Chem.MolFromSmiles(smi) for smi in data]\n",
    "    \n",
    "    if use_parallel:\n",
    "        try:            \n",
    "            with ProcessPoolExecutor() as executor:\n",
    "                results = list(executor.map(get_fingerprints, mols))\n",
    "        except Exception as e:\n",
    "            print(f\"Parallel processing failed due to: {e}. Falling back to sequential processing.\")\n",
    "            use_parallel = False\n",
    "    \n",
    "    if not use_parallel:\n",
    "        results = [get_fingerprints(mol) for mol in mols]\n",
    "    \n",
    "    ECFP, MACCS, AvalonFP = zip(*results)\n",
    "    \n",
    "    ECFP_container = np.vstack([arr for arr in ECFP if arr is not None])\n",
    "    MACCS_container = np.zeros((len(MACCS), LEN_OF_MA), dtype=int)\n",
    "    AvalonFP_container = np.vstack([arr for arr in AvalonFP if arr is not None])\n",
    "\n",
    "    for i, fp in enumerate(MACCS):\n",
    "        if fp is not None:\n",
    "            DataStructs.ConvertToNumpyArray(fp, MACCS_container[i])\n",
    "    \n",
    "    return mols, ECFP_container, MACCS_container, AvalonFP_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mol_ws, x_ws, MACCS_ws, AvalonFP_ws = fp_converter(smiles_ws)\n",
    "mol_de, x_de, MACCS_de, AvalonFP_de = fp_converter(smiles_de)\n",
    "# mol_lo, x_lo, MACCS_lo, AvalonFP_lo = fp_converter(smiles_lo)\n",
    "# mol_hu, x_hu, MACCS_hu, AvalonFP_hu = fp_converter(smiles_hu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_to_numpy(*dataframes):\n",
    "    numpy_arrays = [df.to_numpy() if isinstance(df, pd.DataFrame) else df for df in dataframes]\n",
    "    if not all(isinstance(arr, np.ndarray) for arr in numpy_arrays):\n",
    "        raise ValueError(\"All inputs must be either pandas DataFrame or numpy array\")\n",
    "    return np.concatenate(numpy_arrays, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # group_nws = concatenate_to_numpy(x_ws, MACCS_ws, AvalonFP_ws)\n",
    "    group_nde = concatenate_to_numpy(x_de, MACCS_de, AvalonFP_de)\n",
    "    # group_nlo = concatenate_to_numpy(x_lo, MACCS_lo, AvalonFP_lo)\n",
    "    # group_nhu = concatenate_to_numpy(x_hu, MACCS_hu, AvalonFP_hu)\n",
    "    # del x_ws, MACCS_ws, AvalonFP_ws\n",
    "    del x_de, MACCS_de, AvalonFP_de\n",
    "    # del x_lo, MACCS_lo, AvalonFP_lo\n",
    "    # del x_hu, MACCS_hu, AvalonFP_hu\n",
    "    gc.collect()\n",
    "except Exception as e:\n",
    "    print(f\"Error occured: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCHSIZE = 32\n",
    "EPOCHS = 100\n",
    "lr = 0.01\n",
    "decay = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(\n",
    "            units=1024,\n",
    "            activation='relu',\n",
    "            kernel_initializer='glorot_uniform',\n",
    "            kernel_regularizer=regularizers.l2(decay)),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(\n",
    "            units=469,\n",
    "            activation='relu',\n",
    "            kernel_initializer='glorot_uniform',\n",
    "            kernel_regularizer=regularizers.l2(decay)),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(units=1)\n",
    "        ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "                      loss=tf.keras.losses.MeanSquaredError(),\n",
    "                      metrics=[tf.keras.losses.MeanSquaredError(),\n",
    "                               tf.keras.losses.MeanAbsoluteError(),\n",
    "                               tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_history_plot(history):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    if 'val_loss' in history.history:\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    for metric in history.history:\n",
    "        if metric.startswith('val_'):\n",
    "            continue\n",
    "        plt.plot(history.history[metric], label=f'Training {metric}')\n",
    "        val_metric = f'val_{metric}'\n",
    "        if val_metric in history.history:\n",
    "            plt.plot(history.history[val_metric], label=f'Validation {metric}')\n",
    "    \n",
    "    plt.title(f'Model Metrics')\n",
    "    plt.ylabel('Value')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_result(xdata, ydata):\n",
    "    es_callback = tf.keras.callbacks.EarlyStopping(\n",
    "        patience=3,\n",
    "        monitor='val_loss',  \n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    def preprocess_data(xtr, ytr):\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((xtr, ytr))\n",
    "        dataset = dataset.shuffle(buffer_size=len(xtr)).batch(BATCHSIZE).prefetch(tf.data.AUTOTUNE)\n",
    "        return dataset\n",
    "    tf.keras.backend.clear_session()\n",
    "    xtr, xte, ytr, yte = train_test_split(xdata, ydata, test_size=0.2, random_state=42)\n",
    "    xtr, xtev, ytr, ytev = train_test_split(xdata, ydata, test_size=0.2, random_state=42)\n",
    "    model_new = new_model()\n",
    "    train_data = preprocess_data(xtr, ytr)\n",
    "    valid_data = preprocess_data(xtev, ytev)\n",
    "    history = model_new.fit(train_data, epochs=EPOCHS, batch_size=BATCHSIZE, validation_data=valid_data,callbacks=[es_callback],verbose=0)\n",
    "    save_history_plot(history)\n",
    "    ypred = model_new.predict(xte, verbose=0)\n",
    "    record = r2_score(ypred, yte)\n",
    "    tf.keras.backend.clear_session()\n",
    "    del model_new    \n",
    "    gc.collect()\n",
    "    return record\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalization(descriptor):\n",
    "    descriptor = np.asarray(descriptor)\n",
    "    epsilon = 1e-10\n",
    "    max_value = 1e15\n",
    "    descriptor = np.clip(descriptor, -max_value, max_value)\n",
    "    descriptor_custom = np.where(np.abs(descriptor) < epsilon, epsilon, descriptor)\n",
    "    descriptor_log = np.sign(descriptor_custom) * np.log1p(np.abs(descriptor_custom))\n",
    "    descriptor_log = np.nan_to_num(descriptor_log, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    del epsilon\n",
    "    gc.collect()    \n",
    "    return descriptor_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_molecules_parallel(mols, max_workers=4, chunk_size=100):\n",
    "    results = []    \n",
    "    for i in range(0, len(mols), chunk_size):\n",
    "        chunk = mols[i:i + chunk_size]        \n",
    "        with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = [executor.submit(mol3d, mol) for mol in chunk]\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                if result is not None:\n",
    "                    results.append(result)        \n",
    "        gc.collect()    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mols = mol_de\n",
    "mols2 = process_molecules_parallel(mols, max_workers=8)\n",
    "d1 = [Chem.rdMolDescriptors.CalcPMI1(alpha) for alpha in mols2]\n",
    "d2 = [Chem.rdMolDescriptors.CalcPMI2(alpha) for alpha in mols2]\n",
    "d3 = [Chem.rdMolDescriptors.CalcPMI3(alpha) for alpha in mols2]\n",
    "d1 = Normalization(d1)\n",
    "d2 = Normalization(d2)\n",
    "d3 = Normalization(d3)\n",
    "d1 = np.asarray(d1)\n",
    "d2 = np.asarray(d2)\n",
    "d3 = np.asarray(d3)\n",
    "# ###############\n",
    "# try:\n",
    "#     print(type(descriptor),descriptor.shape)\n",
    "# except:\n",
    "#     print(type(descriptor),len(descriptor))\n",
    "# print(f\"samples: {descriptor[:5]}\")\n",
    "data = np.column_stack([d1,d2,d3])\n",
    "descriptor = np.concatenate([group_nde,data], axis=1)\n",
    "# descriptor = np.asarray(descriptor)\n",
    "r1 = learning_result(group_nde, y_de)\n",
    "# new_fps = np.concatenate([group_nde, descriptor.reshape(-1,1)], axis=1)\n",
    "new_fps = np.concatenate([group_nde, descriptor], axis=1)\n",
    "new_fps = np.nan_to_num(new_fps, nan=0.0, posinf=0.0, neginf=0.0).astype('float')\n",
    "r2 = learning_result(new_fps, y_de)\n",
    "# new_fps = np.concatenate((group_nde, descriptor[:,None]), axis=1)\n",
    "new_fps = np.concatenate([group_nde, descriptor], axis=1)\n",
    "new_fps = np.nan_to_num(new_fps, nan=0.0, posinf=0.0, neginf=0.0).astype('float')\n",
    "r3 = learning_result(new_fps, y_de)\n",
    "print(f\"ori     r2score: {r1:.4f}\")\n",
    "print(f\"reshape r2score: {r2:.4f}\")\n",
    "print(f\":None   r2score: {r3:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
