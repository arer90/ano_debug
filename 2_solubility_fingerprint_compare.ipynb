{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import gc\n",
    "import time\n",
    "import subprocess\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, DataStructs, Draw\n",
    "from rdkit import RDConfig\n",
    "from rdkit.Chem import Descriptors, rdMolDescriptors, Lipinski, rdDistGeom, rdPartialCharges\n",
    "from rdkit.Chem.AllChem import GetMorganGenerator\n",
    "from rdkit.DataStructs.cDataStructs import ConvertToNumpyArray\n",
    "from rdkit.Avalon.pyAvalonTools import GetAvalonFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, root_mean_squared_error,mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_path = \"result/2_solubility_fingerprint_compare\"\n",
    "os.makedirs(target_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ws = pd.read_csv('./data/ws496_logS.csv', dtype={'SMILES': 'string'})\n",
    "smiles_ws = data_ws['SMILES']\n",
    "y_ws = data_ws.iloc[:, 2]\n",
    "\n",
    "data_delaney = pd.read_csv('./data/delaney-processed.csv', dtype={'smiles': 'string'})\n",
    "smiles_de = data_delaney['smiles']\n",
    "y_de = data_delaney.iloc[:, 1]\n",
    "\n",
    "data_lovric2020 = pd.read_csv('./data/Lovric2020_logS0.csv', dtype={'isomeric_smiles': 'string'})\n",
    "smiles_lo = data_lovric2020['isomeric_smiles']\n",
    "y_lo = data_lovric2020.iloc[:, 1]\n",
    "\n",
    "data_huuskonen = pd.read_csv('./data/huusk.csv', dtype={'SMILES': 'string'})\n",
    "smiles_hu = data_huuskonen['SMILES']\n",
    "y_hu = data_huuskonen.iloc[:, -1].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mol3d(mol):\n",
    "    mol = Chem.AddHs(mol)\n",
    "    optimization_methods = [\n",
    "        (AllChem.EmbedMolecule, (mol, AllChem.ETKDGv3()), {}),\n",
    "        (AllChem.UFFOptimizeMolecule, (mol,), {'maxIters': 200}),\n",
    "        (AllChem.MMFFOptimizeMolecule, (mol,), {'maxIters': 200})\n",
    "    ]\n",
    "\n",
    "    for method, args, kwargs in optimization_methods:\n",
    "        try:\n",
    "            method(*args, **kwargs)\n",
    "            if mol.GetNumConformers() > 0:\n",
    "                return mol\n",
    "        except ValueError as e:\n",
    "            print(f\"Error: {e} - Trying next optimization method [{method}]\")\n",
    "\n",
    "    print(f\"Invalid mol for 3d {'\\033[94m'}{Chem.MolToSmiles(mol)}{'\\033[0m'} - No conformer generated\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_smiles_to_mol(smiles, fail_folder=None, index=None, yvalue=None):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        print(f\"[convert_smiles_to_mol] Cannot convert {smiles} to Mols\")\n",
    "        return None, {\"smiles\": smiles, \"y_value\": yvalue, \"error\": \"Invalid SMILES\"}\n",
    "\n",
    "    try:\n",
    "        Chem.Kekulize(mol, clearAromaticFlags=True)\n",
    "        isomeric_smiles = Chem.MolToSmiles(mol, isomericSmiles=True)\n",
    "        mol = Chem.MolFromSmiles(isomeric_smiles)\n",
    "    except Exception as e:\n",
    "        print(f\"[convert_smiles_to_mol] failed {smiles} isomeric_smiles by {e}\")\n",
    "        if fail_folder and index is not None:\n",
    "            img_path = os.path.join(fail_folder, f\"mol_{index}.png\")\n",
    "            img = Draw.MolToImage(mol)\n",
    "            img.save(img_path)\n",
    "        return None, {\"smiles\": smiles, \"y_value\": yvalue, \"error\": f\"Isomeric SMILES error: {e}\"}\n",
    "\n",
    "    try:\n",
    "        Chem.SanitizeMol(mol)\n",
    "    except Exception as e:\n",
    "        print(f\"[convert_smiles_to_mol] failed {smiles} SanitizeMol by {e}\")\n",
    "        if fail_folder and index is not None:\n",
    "            img_path = os.path.join(fail_folder, f\"mol_{index}.png\")\n",
    "            img = Draw.MolToImage(mol)\n",
    "            img.save(img_path)\n",
    "        return None, {\"smiles\": smiles, \"y_value\": yvalue, \"error\": f\"SanitizeMol error: {e}\"}\n",
    "\n",
    "    return mol, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_smiles(smiles, yvalue, fail_folder, index):\n",
    "    mol, error = convert_smiles_to_mol(smiles, fail_folder, index, yvalue)\n",
    "    if error:\n",
    "        return None, None, error\n",
    "\n",
    "    mol_3d = mol3d(mol)\n",
    "    if mol_3d:\n",
    "        return smiles, yvalue, None\n",
    "    else:\n",
    "        img_path = os.path.join(fail_folder, f\"mol_{index}.png\")\n",
    "        img = Draw.MolToImage(mol)\n",
    "        img.save(img_path)\n",
    "        return None, None, {\"smiles\": smiles, \"y_value\": yvalue}\n",
    "\n",
    "def process_dataset(smiles_list, y_values, dataset_name, target_path=\"result\", max_workers=None):\n",
    "    start = time.time()\n",
    "    valid_smiles, valid_y = [], []\n",
    "    error_smiles_list = []\n",
    "    fail_folder = f\"{target_path}/failed/{dataset_name}\"\n",
    "    os.makedirs(fail_folder, exist_ok=True)\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [\n",
    "            executor.submit(process_smiles, smiles, yvalue, fail_folder, i)\n",
    "            for i, (smiles, yvalue) in enumerate(zip(smiles_list, y_values))\n",
    "        ]\n",
    "        for future in as_completed(futures):\n",
    "            smiles, yvalue, error = future.result()\n",
    "            if error:\n",
    "                error_smiles_list.append(error)\n",
    "            elif smiles is not None and yvalue is not None:\n",
    "                valid_smiles.append(smiles)\n",
    "                valid_y.append(yvalue)\n",
    "\n",
    "    if error_smiles_list:\n",
    "        error_df = pd.DataFrame(error_smiles_list)\n",
    "        error_df.to_csv(os.path.join(fail_folder, \"failed_smiles.csv\"), index=False)\n",
    "    print(f\" [{dataset_name:<10}] : {time.time()-start:.4f} sec\")\n",
    "    return valid_smiles, valid_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_ws, y_ws = process_dataset(smiles_ws, y_ws, \"ws496\", target_path)\n",
    "smiles_de, y_de = process_dataset(smiles_de, y_de, \"delaney\", target_path)\n",
    "smiles_lo, y_lo = process_dataset(smiles_lo, y_lo, \"Lovric2020_logS0\", target_path)\n",
    "smiles_hu, y_hu = process_dataset(smiles_hu, y_hu, \"huusk\", target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEN_OF_FF = 2048\n",
    "LEN_OF_MA = 167\n",
    "LEN_OF_AV = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fingerprints(mol):\n",
    "    if mol is None:\n",
    "        return None, None, None\n",
    "    \n",
    "    morgan_generator = GetMorganGenerator(radius=2, fpSize=LEN_OF_FF)\n",
    "    ecfp = morgan_generator.GetFingerprint(mol)\n",
    "    ecfp_array = np.zeros((LEN_OF_FF,),dtype=int)\n",
    "    DataStructs.ConvertToNumpyArray(ecfp, ecfp_array)\n",
    "    \n",
    "    maccs = Chem.rdMolDescriptors.GetMACCSKeysFingerprint(mol)\n",
    "\n",
    "    avalon_fp = GetAvalonFP(mol)\n",
    "    avalon_array = np.zeros((LEN_OF_AV,),dtype=int)\n",
    "    DataStructs.ConvertToNumpyArray(avalon_fp, avalon_array)\n",
    "    \n",
    "    return ecfp_array, maccs, avalon_array\n",
    "\n",
    "def fp_converter(data, use_parallel=True):\n",
    "    mols = [Chem.MolFromSmiles(smi) for smi in data]\n",
    "    \n",
    "    if use_parallel:\n",
    "        try:            \n",
    "            with ProcessPoolExecutor() as executor:\n",
    "                results = list(executor.map(get_fingerprints, mols))\n",
    "        except Exception as e:\n",
    "            print(f\"Parallel processing failed due to: {e}. Falling back to sequential processing.\")\n",
    "            use_parallel = False\n",
    "    \n",
    "    if not use_parallel:\n",
    "        results = [get_fingerprints(mol) for mol in mols]\n",
    "    \n",
    "    ECFP, MACCS, AvalonFP = zip(*results)\n",
    "    \n",
    "    ECFP_container = np.vstack([arr for arr in ECFP if arr is not None])\n",
    "    MACCS_container = np.zeros((len(MACCS), LEN_OF_MA), dtype=int)\n",
    "    AvalonFP_container = np.vstack([arr for arr in AvalonFP if arr is not None])\n",
    "\n",
    "    for i, fp in enumerate(MACCS):\n",
    "        if fp is not None:\n",
    "            DataStructs.ConvertToNumpyArray(fp, MACCS_container[i])\n",
    "    \n",
    "    return mols, ECFP_container, MACCS_container, AvalonFP_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mol_ws, x_ws, MACCS_ws, AvalonFP_ws = fp_converter(smiles_ws)\n",
    "mol_de, x_de, MACCS_de, AvalonFP_de = fp_converter(smiles_de)\n",
    "mol_lo, x_lo, MACCS_lo, AvalonFP_lo = fp_converter(smiles_lo)\n",
    "mol_hu, x_hu, MACCS_hu, AvalonFP_hu = fp_converter(smiles_hu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Morgan Fingerprint (ECFP)\n",
    "xtr_ws1, xte_ws1, ytr_ws1, yte_ws1 = train_test_split(x_ws, y_ws, test_size=0.2,random_state=42)\n",
    "xtr_de1, xte_de1, ytr_de1, yte_de1 = train_test_split(x_de, y_de, test_size=0.2,random_state=42)\n",
    "xtr_lo1, xte_lo1, ytr_lo1, yte_lo1 = train_test_split(x_lo, y_lo, test_size=0.2,random_state=42)\n",
    "xtr_hu1, xte_hu1, ytr_hu1, yte_hu1 = train_test_split(x_hu, y_hu, test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MACCS Fingerprint\n",
    "xtr_ws2, xte_ws2, ytr_ws2, yte_ws2 = train_test_split(MACCS_ws, y_ws, test_size=0.2,random_state=42)\n",
    "xtr_de2, xte_de2, ytr_de2, yte_de2 = train_test_split(MACCS_de, y_de, test_size=0.2,random_state=42)\n",
    "xtr_lo2, xte_lo2, ytr_lo2, yte_lo2 = train_test_split(MACCS_lo, y_lo, test_size=0.2,random_state=42)\n",
    "xtr_hu2, xte_hu2, ytr_hu2, yte_hu2 = train_test_split(MACCS_hu, y_hu, test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Avalon Fingerprint\n",
    "xtr_ws3, xte_ws3, ytr_ws3, yte_ws3 = train_test_split(AvalonFP_ws, y_ws, test_size=0.2,random_state=42)\n",
    "xtr_de3, xte_de3, ytr_de3, yte_de3 = train_test_split(AvalonFP_de, y_de, test_size=0.2,random_state=42)\n",
    "xtr_lo3, xte_lo3, ytr_lo3, yte_lo3 = train_test_split(AvalonFP_lo, y_lo, test_size=0.2,random_state=42)\n",
    "xtr_hu3, xte_hu3, ytr_hu3, yte_hu3 = train_test_split(AvalonFP_hu, y_hu, test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_to_numpy(*dataframes):\n",
    "    numpy_arrays = [df.to_numpy() if isinstance(df, pd.DataFrame) else df for df in dataframes]\n",
    "    if not all(isinstance(arr, np.ndarray) for arr in numpy_arrays):\n",
    "        raise ValueError(\"All inputs must be either pandas DataFrame or numpy array\")\n",
    "    return np.concatenate(numpy_arrays, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ECFP + MACCS\n",
    "group_nws1 = concatenate_to_numpy(x_ws, MACCS_ws)\n",
    "group_nde1 = concatenate_to_numpy(x_de, MACCS_de)\n",
    "group_nlo1 = concatenate_to_numpy(x_lo, MACCS_lo)\n",
    "group_nhu1 = concatenate_to_numpy(x_hu, MACCS_hu)\n",
    "\n",
    "xtr_ws12, xte_ws12, ytr_ws12, yte_ws12 = train_test_split(group_nws1, y_ws, test_size=0.2,random_state=42)\n",
    "xtr_de12, xte_de12, ytr_de12, yte_de12 = train_test_split(group_nde1, y_de, test_size=0.2,random_state=42)\n",
    "xtr_lo12, xte_lo12, ytr_lo12, yte_lo12 = train_test_split(group_nlo1, y_lo, test_size=0.2,random_state=42)\n",
    "xtr_hu12, xte_hu12, ytr_hu12, yte_hu12 = train_test_split(group_nhu1, y_hu, test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ECFP + Avalon\n",
    "group_nws2 = concatenate_to_numpy(x_ws, AvalonFP_ws)\n",
    "group_nde2 = concatenate_to_numpy(x_de, AvalonFP_de)\n",
    "group_nlo2 = concatenate_to_numpy(x_lo, AvalonFP_lo)\n",
    "group_nhu2 = concatenate_to_numpy(x_hu, AvalonFP_hu)\n",
    "\n",
    "xtr_ws13, xte_ws13, ytr_ws13, yte_ws13 = train_test_split(group_nws2, y_ws, test_size=0.2,random_state=42)\n",
    "xtr_de13, xte_de13, ytr_de13, yte_de13 = train_test_split(group_nde2, y_de, test_size=0.2,random_state=42)\n",
    "xtr_lo13, xte_lo13, ytr_lo13, yte_lo13 = train_test_split(group_nlo2, y_lo, test_size=0.2,random_state=42)\n",
    "xtr_hu13, xte_hu13, ytr_hu13, yte_hu13 = train_test_split(group_nhu2, y_hu, test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MACCS + Avalon\n",
    "group_nws3 = concatenate_to_numpy(MACCS_ws, AvalonFP_ws)\n",
    "group_nde3 = concatenate_to_numpy(MACCS_de, AvalonFP_de)\n",
    "group_nlo3 = concatenate_to_numpy(MACCS_lo, AvalonFP_lo)\n",
    "group_nhu3 = concatenate_to_numpy(MACCS_hu, AvalonFP_hu)\n",
    "\n",
    "xtr_ws23, xte_ws23, ytr_ws23, yte_ws23 = train_test_split(group_nws3, y_ws, test_size=0.2,random_state=42)\n",
    "xtr_de23, xte_de23, ytr_de23, yte_de23 = train_test_split(group_nde3, y_de, test_size=0.2,random_state=42)\n",
    "xtr_lo23, xte_lo23, ytr_lo23, yte_lo23 = train_test_split(group_nlo3, y_lo, test_size=0.2,random_state=42)\n",
    "xtr_hu23, xte_hu23, ytr_hu23, yte_hu23 = train_test_split(group_nhu3, y_hu, test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ECFP + MACCS + Avalon\n",
    "fgroup_nws = concatenate_to_numpy(x_ws, MACCS_ws, AvalonFP_ws)\n",
    "fgroup_nde = concatenate_to_numpy(x_de, MACCS_de, AvalonFP_de)\n",
    "fgroup_nlo = concatenate_to_numpy(x_lo, MACCS_lo, AvalonFP_lo)\n",
    "fgroup_nhu = concatenate_to_numpy(x_hu, MACCS_hu, AvalonFP_hu)\n",
    "\n",
    "xtr_wsf, xte_wsf, ytr_wsf, yte_wsf = train_test_split(fgroup_nws, y_ws, test_size=0.2,random_state=42)\n",
    "xtr_def, xte_def, ytr_def, yte_def = train_test_split(fgroup_nde, y_de, test_size=0.2,random_state=42)\n",
    "xtr_lof, xte_lof, ytr_lof, yte_lof = train_test_split(fgroup_nlo, y_lo, test_size=0.2,random_state=42)\n",
    "xtr_huf, xte_huf, ytr_huf, yte_huf = train_test_split(fgroup_nhu, y_hu, test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCHSIZE = 32\n",
    "EPOCHS = 100\n",
    "lr = 0.0001\n",
    "decay = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_inference_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(\n",
    "            units=1024,\n",
    "            activation='relu',\n",
    "            kernel_initializer='glorot_uniform',\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(decay)),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(\n",
    "            units=469,\n",
    "            activation='relu',\n",
    "            kernel_initializer='glorot_uniform',\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(decay)),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(units=1)\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def save_model():\n",
    "    model = new_inference_model()\n",
    "    model_json = model.to_json()\n",
    "    os.makedirs(\"save_model\", exist_ok=True)  # Ensure the directory exists\n",
    "    with open(\"save_model/model_config.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights(\"save_model/model_weights.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment settings for optimal performance\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TensorFlow INFO and WARNING messages\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=2 --tf_xla_enable_xla_devices'\n",
    "os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=/usr/local/cuda --xla_gpu_force_compilation_parallelism=1'\n",
    "\n",
    "import logging\n",
    "class FilterNUMA(logging.Filter):\n",
    "    def filter(self, record):\n",
    "        return \"NUMA\" not in record.getMessage() and \"XLA service\" not in record.getMessage()\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "for handler in logger.handlers:\n",
    "    handler.addFilter(FilterNUMA())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import subprocess\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_study_datasets(xtr_list, ytr_list):\n",
    "    models = []\n",
    "    \n",
    "    fps_file = 'new_fps.npy'\n",
    "    y_true_file = 'y_true.npy'\n",
    "    \n",
    "    for i in range(len(xtr_list)):\n",
    "        np.save(fps_file, xtr_list[i])\n",
    "        np.save(y_true_file, ytr_list[i])\n",
    "        \n",
    "        save_model()\n",
    "        \n",
    "        result = subprocess.run(['python3', './extra_code/basic_model.py', \n",
    "                                 str(BATCHSIZE), str(EPOCHS),\n",
    "                                 fps_file, y_true_file],\n",
    "                                stdout=subprocess.PIPE, \n",
    "                                stderr=subprocess.PIPE, \n",
    "                                text=True)\n",
    "    \n",
    "        if result.stderr:\n",
    "            sys.stderr.write(result.stderr)\n",
    "        \n",
    "        warning_patterns = [\n",
    "            r'WARNING: All log messages before absl::InitializeLog\\(\\) is called are written to STDERR',\n",
    "            r'could not open file to read NUMA node',\n",
    "            r'Your kernel may have been built without NUMA support',\n",
    "            r'XLA service .* initialized for platform (CUDA|Host)',\n",
    "            r'StreamExecutor device \\(0\\):',\n",
    "            r'Created device /job:localhost/replica:0/task:0/device:GPU:0',\n",
    "            r'Using CUDA malloc Async allocator for GPU',\n",
    "            r'Could not identify NUMA node of platform GPU',\n",
    "        ]\n",
    "\n",
    "        filtered_stderr = \"\\n\".join([\n",
    "            line for line in result.stderr.splitlines()\n",
    "            if not any(re.search(pattern, line) for pattern in warning_patterns)\n",
    "        ])\n",
    "        \n",
    "        if filtered_stderr:\n",
    "            print(f\"[{i}] Filtered stderr:\")\n",
    "            print(filtered_stderr)\n",
    "        print(f\"Return code: {result.returncode}\")\n",
    "\n",
    "        if result.returncode != 0 and filtered_stderr and not all('Your kernel may have been built without NUMA support' in line for line in filtered_stderr.split('\\n')):\n",
    "            raise ValueError(f\"[{i}] Error during learning result process: {filtered_stderr}\")\n",
    "        \n",
    "        try:\n",
    "            trained_model = tf.keras.models.load_model('save_model/trained_model.keras')\n",
    "            models.append(trained_model)\n",
    "            print(f\"Model {i+1} loaded successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"[{i}] Error loading model: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    os.remove(fps_file)\n",
    "    os.remove(y_true_file)\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtr_ws=[xtr_ws1, xtr_ws2, xtr_ws3, xtr_ws12, xtr_ws13, xtr_ws23, xtr_wsf]\n",
    "ytr_ws=[ytr_ws1, ytr_ws2, ytr_ws3, ytr_ws12, ytr_ws13, ytr_ws23, ytr_wsf]\n",
    "xte_ws=[xte_ws1, xte_ws2, xte_ws3, xte_ws12, xte_ws13, xte_ws23, xte_wsf]\n",
    "yte_ws=[yte_ws1, yte_ws2, yte_ws3, yte_ws12, yte_ws13, yte_ws23, yte_wsf]\n",
    "res_ws = model_study_datasets(xtr_ws,ytr_ws)\n",
    "# 7m 30s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtr_de=[xtr_de1, xtr_de2, xtr_de3, xtr_de12, xtr_de13, xtr_de23, xtr_def]\n",
    "ytr_de=[ytr_de1, ytr_de2, ytr_de3, ytr_de12, ytr_de13, ytr_de23, ytr_def]\n",
    "xte_de=[xte_de1, xte_de2, xte_de3, xte_de12, xte_de13, xte_de23, xte_def]\n",
    "yte_de=[yte_de1, yte_de2, yte_de3, yte_de12, yte_de13, yte_de23, yte_def]\n",
    "res_de = model_study_datasets(xtr_de,ytr_de)\n",
    "#12m 43s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtr_lo=[xtr_lo1, xtr_lo2, xtr_lo3, xtr_lo12, xtr_lo13, xtr_lo23, xtr_lof]\n",
    "ytr_lo=[ytr_lo1, ytr_lo2, ytr_lo3, ytr_lo12, ytr_lo13, ytr_lo23, ytr_lof]\n",
    "xte_lo=[xte_lo1, xte_lo2, xte_lo3, xte_lo12, xte_lo13, xte_lo23, xte_lof]\n",
    "yte_lo=[yte_lo1, yte_lo2, yte_lo3, yte_lo12, yte_lo13, yte_lo23, yte_lof]\n",
    "res_lo = model_study_datasets(xtr_lo,ytr_lo)\n",
    "#10m 9.2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtr_hu=[xtr_hu1, xtr_hu2, xtr_hu3, xtr_hu12, xtr_hu13, xtr_hu23, xtr_huf]\n",
    "ytr_hu=[ytr_hu1, ytr_hu2, ytr_hu3, ytr_hu12, ytr_hu13, ytr_hu23, ytr_huf]\n",
    "xte_hu=[xte_hu1, xte_hu2, xte_hu3, xte_hu12, xte_hu13, xte_hu23, xte_huf]\n",
    "yte_hu=[yte_hu1, yte_hu2, yte_hu3, yte_hu12, yte_hu13, yte_hu23, yte_huf]\n",
    "res_hu = model_study_datasets(xtr_hu,ytr_hu)\n",
    "#13m 24s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(dataset, model_name, ypre,yte):\n",
    "    r2 = r2_score(yte,ypre)\n",
    "    mae  = mean_absolute_error(yte,ypre)\n",
    "    mse  = mean_squared_error(yte,ypre)\n",
    "    rmse = root_mean_squared_error(yte,ypre)\n",
    "    print(f\"[data : {dataset} ][model : {model_name} ] = r2 : {r2:.5f}, mae : {mae:.5f}, mse : {mse:.5f}, rmse : {rmse:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics('ws','ECEP      ', res_ws[0].predict(xte_ws[0], verbose=0), yte_ws[0])\n",
    "print_metrics('ws','MACCS     ', res_ws[1].predict(xte_ws[1], verbose=0), yte_ws[1])\n",
    "print_metrics('ws','Avalon    ', res_ws[2].predict(xte_ws[2], verbose=0), yte_ws[2])\n",
    "print_metrics('ws','ECEP_MACCS', res_ws[3].predict(xte_ws[3], verbose=0), yte_ws[3])\n",
    "print_metrics('ws','ECEP_Ava. ', res_ws[4].predict(xte_ws[4], verbose=0), yte_ws[4])\n",
    "print_metrics('ws','MACCS_Ava.', res_ws[5].predict(xte_ws[5], verbose=0), yte_ws[5])\n",
    "print_metrics('ws','ECFP+MACCS+Ava.', res_ws[6].predict(xte_ws[6], verbose=0), yte_ws[6])\n",
    "\n",
    "# [data : ws ][model : ECEP       ] = r2 : 0.66232, mae : 0.83593, mse : 1.10171, rmse : 1.04962\n",
    "# [data : ws ][model : MACCS      ] = r2 : 0.74583, mae : 0.71737, mse : 0.82926, rmse : 0.91064\n",
    "# [data : ws ][model : Avalon     ] = r2 : 0.71877, mae : 0.75613, mse : 0.91754, rmse : 0.95788\n",
    "# [data : ws ][model : ECEP_MACCS ] = r2 : 0.77450, mae : 0.63215, mse : 0.73572, rmse : 0.85774\n",
    "# [data : ws ][model : ECEP_Ava.  ] = r2 : 0.74313, mae : 0.67417, mse : 0.83806, rmse : 0.91546\n",
    "# [data : ws ][model : MACCS_Ava. ] = r2 : 0.77575, mae : 0.65553, mse : 0.73164, rmse : 0.85536\n",
    "# [data : ws ][model : ECFP+MACCS+Ava. ] = r2 : 0.78273, mae : 0.62530, mse : 0.70886, rmse : 0.84194"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics('de','ECEP      ', res_de[0].predict(xte_de[0], verbose=0), yte_de[0])\n",
    "print_metrics('de','MACCS     ', res_de[1].predict(xte_de[1], verbose=0), yte_de[1])\n",
    "print_metrics('de','Avalon    ', res_de[2].predict(xte_de[2], verbose=0), yte_de[2])\n",
    "print_metrics('de','ECEP_MACCS', res_de[3].predict(xte_de[3], verbose=0), yte_de[3])\n",
    "print_metrics('de','ECEP_Ava. ', res_de[4].predict(xte_de[4], verbose=0), yte_de[4])\n",
    "print_metrics('de','MACCS_Ava.', res_de[5].predict(xte_de[5], verbose=0), yte_de[5])\n",
    "print_metrics('de','ECFP+MACCS+Ava.', res_de[6].predict(xte_de[6], verbose=0), yte_de[6])\n",
    "\n",
    "# [data : de ][model : ECEP       ] = r2 : 0.83667, mae : 0.46861, mse : 0.48737, rmse : 0.69812\n",
    "# [data : de ][model : MACCS      ] = r2 : 0.79751, mae : 0.49282, mse : 0.60425, rmse : 0.77734\n",
    "# [data : de ][model : Avalon     ] = r2 : 0.85147, mae : 0.41674, mse : 0.44323, rmse : 0.66576\n",
    "# [data : de ][model : ECEP_MACCS ] = r2 : 0.85653, mae : 0.38113, mse : 0.42812, rmse : 0.65431\n",
    "# [data : de ][model : ECEP_Ava.  ] = r2 : 0.88713, mae : 0.35384, mse : 0.33680, rmse : 0.58035\n",
    "# [data : de ][model : MACCS_Ava. ] = r2 : 0.88132, mae : 0.34731, mse : 0.35414, rmse : 0.59509\n",
    "# [data : de ][model : ECFP+MACCS+Ava. ] = r2 : 0.88028, mae : 0.34027, mse : 0.35725, rmse : 0.59770\n",
    "\n",
    "# [data : de ][model : ECEP       ] = r2 : 0.83681, mae : 0.48289, mse : 0.48696, rmse : 0.69783\n",
    "# [data : de ][model : MACCS      ] = r2 : 0.81288, mae : 0.46656, mse : 0.55837, rmse : 0.74724\n",
    "# [data : de ][model : Avalon     ] = r2 : 0.86393, mae : 0.38842, mse : 0.40602, rmse : 0.63720\n",
    "# [data : de ][model : ECEP_MACCS ] = r2 : 0.86069, mae : 0.38990, mse : 0.41570, rmse : 0.64475\n",
    "# [data : de ][model : ECEP_Ava.  ] = r2 : 0.86526, mae : 0.39938, mse : 0.40208, rmse : 0.63410\n",
    "# [data : de ][model : MACCS_Ava. ] = r2 : 0.87254, mae : 0.36285, mse : 0.38035, rmse : 0.61673\n",
    "# [data : de ][model : ECFP+MACCS+Ava. ] = r2 : 0.89274, mae : 0.32713, mse : 0.32007, rmse : 0.56575"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics('lo','ECEP      ', res_lo[0].predict(xte_lo[0], verbose=0), yte_lo[0])\n",
    "print_metrics('lo','MACCS     ', res_lo[1].predict(xte_lo[1], verbose=0), yte_lo[1])\n",
    "print_metrics('lo','Avalon    ', res_lo[2].predict(xte_lo[2], verbose=0), yte_lo[2])\n",
    "print_metrics('lo','ECEP_MACCS', res_lo[3].predict(xte_lo[3], verbose=0), yte_lo[3])\n",
    "print_metrics('lo','ECEP_Ava. ', res_lo[4].predict(xte_lo[4], verbose=0), yte_lo[4])\n",
    "print_metrics('lo','MACCS_Ava.', res_lo[5].predict(xte_lo[5], verbose=0), yte_lo[5])\n",
    "print_metrics('lo','ECFP+MACCS+Ava.', res_lo[6].predict(xte_lo[6], verbose=0), yte_lo[6])\n",
    "\n",
    "# [data : lo ][model : ECEP       ] = r2 : 0.62102, mae : 0.73440, mse : 1.00307, rmse : 1.00153\n",
    "# [data : lo ][model : MACCS      ] = r2 : 0.59578, mae : 0.68067, mse : 1.06987, rmse : 1.03435\n",
    "# [data : lo ][model : Avalon     ] = r2 : 0.68422, mae : 0.67421, mse : 0.83580, rmse : 0.91422\n",
    "# [data : lo ][model : ECEP_MACCS ] = r2 : 0.68492, mae : 0.64230, mse : 0.83395, rmse : 0.91321\n",
    "# [data : lo ][model : ECEP_Ava.  ] = r2 : 0.69790, mae : 0.64407, mse : 0.79957, rmse : 0.89419\n",
    "# [data : lo ][model : MACCS_Ava. ] = r2 : 0.70558, mae : 0.61507, mse : 0.77926, rmse : 0.88276\n",
    "# [data : lo ][model : ECFP+MACCS+Ava. ] = r2 : 0.71277, mae : 0.62492, mse : 0.76024, rmse : 0.87192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics('hu','ECEP      ', res_hu[0].predict(xte_hu[0], verbose=0), yte_hu[0])\n",
    "print_metrics('hu','MACCS     ', res_hu[1].predict(xte_hu[1], verbose=0), yte_hu[1])\n",
    "print_metrics('hu','Avalon    ', res_hu[2].predict(xte_hu[2], verbose=0), yte_hu[2])\n",
    "print_metrics('hu','ECEP_MACCS', res_hu[3].predict(xte_hu[3], verbose=0), yte_hu[3])\n",
    "print_metrics('hu','ECEP_Ava. ', res_hu[4].predict(xte_hu[4], verbose=0), yte_hu[4])\n",
    "print_metrics('hu','MACCS_Ava.', res_hu[5].predict(xte_hu[5], verbose=0), yte_hu[5])\n",
    "print_metrics('hu','ECFP+MACCS+Ava.', res_hu[6].predict(xte_hu[6], verbose=0), yte_hu[6])\n",
    "\n",
    "# [data : hu ][model : ECEP       ] = r2 : 0.74465, mae : 0.79418, mse : 1.13364, rmse : 1.06472\n",
    "# [data : hu ][model : MACCS      ] = r2 : 0.81311, mae : 0.67737, mse : 0.82969, rmse : 0.91088\n",
    "# [data : hu ][model : Avalon     ] = r2 : 0.83078, mae : 0.64302, mse : 0.75128, rmse : 0.86676\n",
    "# [data : hu ][model : ECEP_MACCS ] = r2 : 0.84665, mae : 0.61117, mse : 0.68083, rmse : 0.82512\n",
    "# [data : hu ][model : ECEP_Ava.  ] = r2 : 0.85118, mae : 0.58893, mse : 0.66071, rmse : 0.81284\n",
    "# [data : hu ][model : MACCS_Ava. ] = r2 : 0.85636, mae : 0.57487, mse : 0.63769, rmse : 0.79856\n",
    "# [data : hu ][model : ECFP+MACCS+Ava. ] = r2 : 0.86441, mae : 0.57482, mse : 0.60195, rmse : 0.77585"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred_de = [res_de[0].predict(xte_de[0], verbose=0),\n",
    "            res_de[1].predict(xte_de[1], verbose=0),\n",
    "            res_de[2].predict(xte_de[2], verbose=0),\n",
    "            res_de[3].predict(xte_de[3], verbose=0),\n",
    "            res_de[4].predict(xte_de[4], verbose=0),\n",
    "            res_de[5].predict(xte_de[5], verbose=0),\n",
    "            res_de[6].predict(xte_de[6], verbose=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred_ws = [res_ws[0].predict(xte_ws[0], verbose=0),\n",
    "            res_ws[1].predict(xte_ws[1], verbose=0),\n",
    "            res_ws[2].predict(xte_ws[2], verbose=0),\n",
    "            res_ws[3].predict(xte_ws[3], verbose=0),\n",
    "            res_ws[4].predict(xte_ws[4], verbose=0),\n",
    "            res_ws[5].predict(xte_ws[5], verbose=0),\n",
    "            res_ws[6].predict(xte_ws[6], verbose=0)]\n",
    "\n",
    "ypred_de = [res_de[0].predict(xte_de[0], verbose=0),\n",
    "            res_de[1].predict(xte_de[1], verbose=0),\n",
    "            res_de[2].predict(xte_de[2], verbose=0),\n",
    "            res_de[3].predict(xte_de[3], verbose=0),\n",
    "            res_de[4].predict(xte_de[4], verbose=0),\n",
    "            res_de[5].predict(xte_de[5], verbose=0),\n",
    "            res_de[6].predict(xte_de[6], verbose=0)]\n",
    "\n",
    "ypred_lo = [res_lo[0].predict(xte_lo[0], verbose=0),\n",
    "            res_lo[1].predict(xte_lo[1], verbose=0),\n",
    "            res_lo[2].predict(xte_lo[2], verbose=0),\n",
    "            res_lo[3].predict(xte_lo[3], verbose=0),\n",
    "            res_lo[4].predict(xte_lo[4], verbose=0),\n",
    "            res_lo[5].predict(xte_lo[5], verbose=0),\n",
    "            res_lo[6].predict(xte_lo[6], verbose=0)]\n",
    "\n",
    "ypred_hu = [res_hu[0].predict(xte_hu[0], verbose=0),\n",
    "            res_hu[1].predict(xte_hu[1], verbose=0),\n",
    "            res_hu[2].predict(xte_hu[2], verbose=0),\n",
    "            res_hu[3].predict(xte_hu[3], verbose=0),\n",
    "            res_hu[4].predict(xte_hu[4], verbose=0),\n",
    "            res_hu[5].predict(xte_hu[5], verbose=0),\n",
    "            res_hu[6].predict(xte_hu[6], verbose=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_res_fingerprint(ypre,yte,name,target_path):\n",
    "    fig = plt.figure(constrained_layout=True, figsize=(15, 5))\n",
    "    fig.suptitle('{0} Fingerprint Compare.'.format(name), fontsize=16)\n",
    "    ax0 = fig.add_subplot(131)\n",
    "    ax0.set_title('ECFP, MACCS, Avalon')\n",
    "    ax0.plot(yte[0],yte[0],'k-')\n",
    "    ax0.plot(yte[0],ypre[0],'b*',label='%s R2 = %.3f %%' % ('ECFP     ',r2_score(yte[0],ypre[0])))\n",
    "    ax0.plot(yte[1],ypre[1],'r*',label='%s R2 = %.3f %%' % ('MACCS  ',r2_score(yte[1],ypre[1])))\n",
    "    ax0.plot(yte[2],ypre[2],'c*',label='%s R2 = %.3f %%' % ('Avalon  ',r2_score(yte[2],ypre[2])))\n",
    "    ax0.legend(loc='upper left')\n",
    "    ax0.set_ylabel('Y Truth')\n",
    "    ax0.set_xlabel('Y Prediction')\n",
    "    \n",
    "    ax1 = fig.add_subplot(132)\n",
    "    ax1.set_title('ECFP+MACCS, ECFP+Avalon, MACCS+Avalon')\n",
    "    ax1.plot(yte[0],yte[0],'k-')\n",
    "    ax1.plot(yte[3],ypre[3],'b*',label='%s R2 = %.3f %%' % ('ECEP_MACCS   ',r2_score(yte[3],ypre[3])))\n",
    "    ax1.plot(yte[4],ypre[4],'r*',label='%s R2 = %.3f %%' % ('ECEP_Avalon    ',r2_score(yte[4],ypre[4])))\n",
    "    ax1.plot(yte[5],ypre[5],'c*',label='%s R2 = %.3f %%' % ('MACCS_Avalon ',r2_score(yte[5],ypre[5])))\n",
    "    ax1.legend(loc='upper left')\n",
    "    ax1.set_ylabel('Y Truth')\n",
    "    ax1.set_xlabel('Y Prediction')\n",
    "    \n",
    "    ax2 = fig.add_subplot(133)\n",
    "    ax2.set_title('ECFP+MACCS+Avalon Fingerprints')\n",
    "    ax2.plot(yte[0],yte[0],'k-')\n",
    "    ax2.plot(yte[6],ypre[6],'b*',label='%s R2 = %.3f %%' % ('ECFP+MACCS+Avalon.  ',r2_score(yte[6],ypre[6])))\n",
    "    ax2.legend(loc='upper left')\n",
    "    ax2.set_ylabel('Y Truth')\n",
    "    ax2.set_xlabel('Y Prediction')\n",
    "    plt.show()\n",
    "    fig.savefig(f\"{target_path}/fingerprint_compare_{name}.png\", dpi=300)\n",
    "vis_res_fingerprint(ypred_ws,yte_ws,'ws496',target_path)\n",
    "vis_res_fingerprint(ypred_de,yte_de,'delaney',target_path)\n",
    "vis_res_fingerprint(ypred_lo,yte_lo,'lovrics',target_path)\n",
    "vis_res_fingerprint(ypred_hu,yte_hu,'huuskonen',target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
