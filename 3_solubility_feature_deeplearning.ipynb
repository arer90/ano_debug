{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import gc\n",
    "import time\n",
    "import subprocess\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, DataStructs, Draw\n",
    "from rdkit import RDConfig\n",
    "from rdkit.Chem import Descriptors, rdMolDescriptors, Lipinski, rdDistGeom, rdPartialCharges\n",
    "from rdkit.Chem.AllChem import GetMorganGenerator\n",
    "from rdkit.DataStructs.cDataStructs import ConvertToNumpyArray\n",
    "from rdkit.Avalon.pyAvalonTools import GetAvalonFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, root_mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_path = \"result/3_solubility_descriptor_deeplearning\"\n",
    "os.makedirs(target_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ws = pd.read_csv('./data/ws496_logS.csv', dtype={'SMILES': 'string'})\n",
    "smiles_ws = data_ws['SMILES']\n",
    "y_ws = data_ws.iloc[:, 2]\n",
    "\n",
    "data_delaney = pd.read_csv('./data/delaney-processed.csv', dtype={'smiles': 'string'})\n",
    "smiles_de = data_delaney['smiles']\n",
    "y_de = data_delaney.iloc[:, 1]\n",
    "\n",
    "data_lovric2020 = pd.read_csv('./data/Lovric2020_logS0.csv', dtype={'isomeric_smiles': 'string'})\n",
    "smiles_lo = data_lovric2020['isomeric_smiles']\n",
    "y_lo = data_lovric2020.iloc[:, 1]\n",
    "\n",
    "data_huuskonen = pd.read_csv('./data/huusk.csv', dtype={'SMILES': 'string'})\n",
    "smiles_hu = data_huuskonen['SMILES']\n",
    "y_hu = data_huuskonen.iloc[:, -1].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mol3d(mol):\n",
    "    mol = Chem.AddHs(mol)\n",
    "    optimization_methods = [\n",
    "        (AllChem.EmbedMolecule, (mol, AllChem.ETKDGv3()), {}),\n",
    "        (AllChem.UFFOptimizeMolecule, (mol,), {'maxIters': 200}),\n",
    "        (AllChem.MMFFOptimizeMolecule, (mol,), {'maxIters': 200})\n",
    "    ]\n",
    "\n",
    "    for method, args, kwargs in optimization_methods:\n",
    "        try:\n",
    "            method(*args, **kwargs)\n",
    "            if mol.GetNumConformers() > 0:\n",
    "                return mol\n",
    "        except ValueError as e:\n",
    "            print(f\"Error: {e} - Trying next optimization method [{method}]\")\n",
    "\n",
    "    print(f\"Invalid mol for 3d {'\\033[94m'}{Chem.MolToSmiles(mol)}{'\\033[0m'} - No conformer generated\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_smiles_to_mol(smiles, fail_folder=None, index=None, yvalue=None):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        print(f\"[convert_smiles_to_mol] Cannot convert {smiles} to Mols\")\n",
    "        return None, {\"smiles\": smiles, \"y_value\": yvalue, \"error\": \"Invalid SMILES\"}\n",
    "\n",
    "    try:\n",
    "        Chem.Kekulize(mol, clearAromaticFlags=True)\n",
    "        isomeric_smiles = Chem.MolToSmiles(mol, isomericSmiles=True)\n",
    "        mol = Chem.MolFromSmiles(isomeric_smiles)\n",
    "    except Exception as e:\n",
    "        print(f\"[convert_smiles_to_mol] failed {smiles} isomeric_smiles by {e}\")\n",
    "        if fail_folder and index is not None:\n",
    "            img_path = os.path.join(fail_folder, f\"mol_{index}.png\")\n",
    "            img = Draw.MolToImage(mol)\n",
    "            img.save(img_path)\n",
    "        return None, {\"smiles\": smiles, \"y_value\": yvalue, \"error\": f\"Isomeric SMILES error: {e}\"}\n",
    "\n",
    "    try:\n",
    "        Chem.SanitizeMol(mol)\n",
    "    except Exception as e:\n",
    "        print(f\"[convert_smiles_to_mol] failed {smiles} SanitizeMol by {e}\")\n",
    "        if fail_folder and index is not None:\n",
    "            img_path = os.path.join(fail_folder, f\"mol_{index}.png\")\n",
    "            img = Draw.MolToImage(mol)\n",
    "            img.save(img_path)\n",
    "        return None, {\"smiles\": smiles, \"y_value\": yvalue, \"error\": f\"SanitizeMol error: {e}\"}\n",
    "\n",
    "    return mol, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_smiles(smiles, yvalue, fail_folder, index):\n",
    "    mol, error = convert_smiles_to_mol(smiles, fail_folder, index, yvalue)\n",
    "    if error:\n",
    "        return None, None, error\n",
    "\n",
    "    mol_3d = mol3d(mol)\n",
    "    if mol_3d:\n",
    "        return smiles, yvalue, None\n",
    "    else:\n",
    "        img_path = os.path.join(fail_folder, f\"mol_{index}.png\")\n",
    "        img = Draw.MolToImage(mol)\n",
    "        img.save(img_path)\n",
    "        return None, None, {\"smiles\": smiles, \"y_value\": yvalue}\n",
    "\n",
    "def process_dataset(smiles_list, y_values, dataset_name, target_path=\"result\", max_workers=None):\n",
    "    start = time.time()\n",
    "    valid_smiles, valid_y = [], []\n",
    "    error_smiles_list = []\n",
    "    fail_folder = f\"{target_path}/failed/{dataset_name}\"\n",
    "    os.makedirs(fail_folder, exist_ok=True)\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [\n",
    "            executor.submit(process_smiles, smiles, yvalue, fail_folder, i)\n",
    "            for i, (smiles, yvalue) in enumerate(zip(smiles_list, y_values))\n",
    "        ]\n",
    "        for future in as_completed(futures):\n",
    "            smiles, yvalue, error = future.result()\n",
    "            if error:\n",
    "                error_smiles_list.append(error)\n",
    "            elif smiles is not None and yvalue is not None:\n",
    "                valid_smiles.append(smiles)\n",
    "                valid_y.append(yvalue)\n",
    "\n",
    "    if error_smiles_list:\n",
    "        error_df = pd.DataFrame(error_smiles_list)\n",
    "        error_df.to_csv(os.path.join(fail_folder, \"failed_smiles.csv\"), index=False)\n",
    "    print(f\" [{dataset_name:<10}] : {time.time()-start:.4f} sec\")\n",
    "    return valid_smiles, valid_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_ws, y_ws = process_dataset(smiles_ws, y_ws, \"ws496\", target_path)\n",
    "smiles_de, y_de = process_dataset(smiles_de, y_de, \"delaney\", target_path)\n",
    "smiles_lo, y_lo = process_dataset(smiles_lo, y_lo, \"Lovric2020_logS0\", target_path)\n",
    "smiles_hu, y_hu = process_dataset(smiles_hu, y_hu, \"huusk\", target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEN_OF_FF = 2048\n",
    "LEN_OF_MA = 167\n",
    "LEN_OF_AV = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fingerprints(mol):\n",
    "    if mol is None:\n",
    "        return None, None, None\n",
    "    \n",
    "    morgan_generator = GetMorganGenerator(radius=2, fpSize=LEN_OF_FF)\n",
    "    ecfp = morgan_generator.GetFingerprint(mol)\n",
    "    ecfp_array = np.zeros((LEN_OF_FF,),dtype=int)\n",
    "    DataStructs.ConvertToNumpyArray(ecfp, ecfp_array)\n",
    "    \n",
    "    maccs = Chem.rdMolDescriptors.GetMACCSKeysFingerprint(mol)\n",
    "\n",
    "    avalon_fp = GetAvalonFP(mol)\n",
    "    avalon_array = np.zeros((LEN_OF_AV,),dtype=int)\n",
    "    DataStructs.ConvertToNumpyArray(avalon_fp, avalon_array)\n",
    "    \n",
    "    return ecfp_array, maccs, avalon_array\n",
    "\n",
    "def fp_converter(data, use_parallel=True):\n",
    "    mols = [Chem.MolFromSmiles(smi) for smi in data]\n",
    "    \n",
    "    if use_parallel:\n",
    "        try:            \n",
    "            with ProcessPoolExecutor() as executor:\n",
    "                results = list(executor.map(get_fingerprints, mols))\n",
    "        except Exception as e:\n",
    "            print(f\"Parallel processing failed due to: {e}. Falling back to sequential processing.\")\n",
    "            use_parallel = False\n",
    "    \n",
    "    if not use_parallel:\n",
    "        results = [get_fingerprints(mol) for mol in mols]\n",
    "    \n",
    "    ECFP, MACCS, AvalonFP = zip(*results)\n",
    "    \n",
    "    ECFP_container = np.vstack([arr for arr in ECFP if arr is not None])\n",
    "    MACCS_container = np.zeros((len(MACCS), LEN_OF_MA), dtype=int)\n",
    "    AvalonFP_container = np.vstack([arr for arr in AvalonFP if arr is not None])\n",
    "\n",
    "    for i, fp in enumerate(MACCS):\n",
    "        if fp is not None:\n",
    "            DataStructs.ConvertToNumpyArray(fp, MACCS_container[i])\n",
    "    \n",
    "    return mols, ECFP_container, MACCS_container, AvalonFP_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mol_ws, x_ws, MACCS_ws, AvalonFP_ws = fp_converter(smiles_ws)\n",
    "mol_de, x_de, MACCS_de, AvalonFP_de = fp_converter(smiles_de)\n",
    "mol_lo, x_lo, MACCS_lo, AvalonFP_lo = fp_converter(smiles_lo)\n",
    "mol_hu, x_hu, MACCS_hu, AvalonFP_hu = fp_converter(smiles_hu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCHSIZE = 32\n",
    "EPOCHS = 100\n",
    "lr = 0.01 #0.0001\n",
    "decay = 1e-5 #1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(\n",
    "            units=1024,\n",
    "            activation='relu',\n",
    "            kernel_initializer='glorot_uniform',\n",
    "            kernel_regularizer=regularizers.l2(decay)),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(\n",
    "            units=469,\n",
    "            activation='relu',\n",
    "            kernel_initializer='glorot_uniform',\n",
    "            kernel_regularizer=regularizers.l2(decay)),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(units=1)\n",
    "        ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "                      loss=tf.keras.losses.MeanSquaredError(),\n",
    "                      metrics=[tf.keras.losses.MeanSquaredError(),\n",
    "                               tf.keras.losses.MeanAbsoluteError(),\n",
    "                               tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model\n",
    "\n",
    "def new_inference_model(input_dim):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(\n",
    "            units=1024,\n",
    "            activation='relu',\n",
    "            kernel_initializer='glorot_uniform',\n",
    "            kernel_regularizer=regularizers.l2(decay)),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(\n",
    "            units=469,\n",
    "            activation='relu',\n",
    "            kernel_initializer='glorot_uniform',\n",
    "            kernel_regularizer=regularizers.l2(decay)),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(units=1)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def save_model(x_data):\n",
    "    model_path = \"save_model/full_model.keras\"\n",
    "    if not os.path.exists(model_path):\n",
    "        try:\n",
    "            model = new_inference_model(x_data.shape[1])\n",
    "            os.makedirs(\"save_model\", exist_ok=True)\n",
    "            model.save(model_path)\n",
    "            # print(f\"Model successfully saved to {model_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving model: {e}\")\n",
    "    else:\n",
    "        # print(f\"Model already exists at {model_path}\")\n",
    "        os.remove(model_path)\n",
    "        save_model(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import logging\n",
    "# Environment settings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=2 --tf_xla_enable_xla_devices'\n",
    "os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=/usr/local/cuda --xla_gpu_force_compilation_parallelism=1'\n",
    "\n",
    "# Suppress TensorFlow logging\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_result(xdata, ydata):\n",
    "    BATCHSIZE = 32\n",
    "    EPOCHS = 100\n",
    "    try:\n",
    "        tf.keras.backend.clear_session()\n",
    "        xtr, xte, ytr, yte = train_test_split(xdata, ydata, test_size=0.2, random_state=42)\n",
    "        model_new = new_model()\n",
    "        model_new.fit(xtr, ytr, epochs=EPOCHS, batch_size=BATCHSIZE, validation_split=0.2,verbose=0)\n",
    "        ypred = model_new.predict(xte, verbose=0)\n",
    "        record = r2_score(ypred, yte)\n",
    "    except Exception as e:\n",
    "        print(f\"[learning_result]Error occureed: {e}\")\n",
    "    tf.keras.backend.clear_session()\n",
    "    del model_new    \n",
    "    gc.collect()\n",
    "    return record\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_descriptor(fps, descriptor, descriptor_name, y_true, r2_list, esti_time=None, cpu_calc=False):\n",
    "    try:\n",
    "        new_fps = fps.copy()\n",
    "        start = time.time()\n",
    "        \n",
    "        # Descriptor processing code remains the same...\n",
    "        if descriptor is None:\n",
    "            pass\n",
    "        elif isinstance(descriptor, np.ndarray) and descriptor.ndim >= 2:\n",
    "            try:\n",
    "                new_fps = np.concatenate([new_fps, descriptor], axis=1)\n",
    "                del descriptor\n",
    "            except Exception as e:\n",
    "                print(f\"[-1-] Error occured: {e}\")\n",
    "        elif isinstance(descriptor, list) and isinstance(descriptor[0], np.ndarray):\n",
    "            try:\n",
    "                arrays_1d = [arr[:, None] for arr in descriptor if arr.ndim == 1]\n",
    "                arrays_2d = [arr for arr in descriptor if arr.ndim == 2]\n",
    "                combined_1d = np.concatenate(arrays_1d, axis=1) if arrays_1d else None\n",
    "                combined_2d = np.concatenate(arrays_2d, axis=1) if arrays_2d else None\n",
    "                to_concat = [new_fps] + [arr for arr in [combined_1d, combined_2d] if arr is not None]\n",
    "                new_fps = np.concatenate(to_concat, axis=1)\n",
    "                del descriptor, arrays_1d, arrays_2d\n",
    "                if combined_1d is not None: del combined_1d\n",
    "                if combined_2d is not None: del combined_2d\n",
    "            except Exception as e:\n",
    "                print(f\"[-2-] Error occured: {e}\")\n",
    "        elif isinstance(descriptor, list) and isinstance(descriptor[0], list):\n",
    "            try:\n",
    "                descriptor = np.asarray(descriptor).astype('float')\n",
    "                new_fps = np.concatenate([new_fps, descriptor], axis=1)\n",
    "                del descriptor\n",
    "            except Exception as e:\n",
    "                print(f\"[-3-] Error occured: {e}\")\n",
    "        else:\n",
    "            descriptor = np.asarray(descriptor).astype('float')\n",
    "            new_fps = np.concatenate([new_fps, descriptor[:,None]], axis=1)\n",
    "            del descriptor\n",
    "\n",
    "        try:\n",
    "            new_fps = np.nan_to_num(new_fps, nan=0.0, posinf=0.0, neginf=0.0).astype('float')\n",
    "            y_true = np.asarray(y_true).astype('float')\n",
    "            \n",
    "            # Save the data files with simple names\n",
    "            np.save('new_fps.npy', new_fps)\n",
    "            np.save('y_true.npy', y_true)\n",
    "            \n",
    "            save_model(new_fps)  # Make sure this function exists and works as expected\n",
    "            \n",
    "            # Run the learning process\n",
    "            result = subprocess.run(['python3', './extra_code/learning_process.py', \n",
    "                                   str(BATCHSIZE), str(EPOCHS), \n",
    "                                   str(lr),\n",
    "                                   'new_fps.npy', 'y_true.npy'],\n",
    "                                  stdout=subprocess.PIPE, \n",
    "                                  stderr=subprocess.PIPE, \n",
    "                                  text=True,\n",
    "                                  encoding='utf-8')  # Explicitly specify encoding\n",
    "            stdout_lines = result.stdout.strip().splitlines()\n",
    "\n",
    "            # Find R2 in the stdout output\n",
    "            r2_result = 0.0\n",
    "            for line in stdout_lines:\n",
    "                if line.startswith(\"R2:\"):\n",
    "                    r2_result = float(line.split(\"R2:\")[1].strip().split()[0])\n",
    "                    break\n",
    "            \n",
    "            if result.stderr:\n",
    "                print(f\"[{descriptor_name}] stderr output:\", result.stderr)\n",
    "\n",
    "            # Store the result\n",
    "            r2_list[descriptor_name] = r2_result\n",
    "            \n",
    "            duration_time = time.time() - start\n",
    "            if esti_time is not None:\n",
    "                esti_time[descriptor_name] = duration_time\n",
    "\n",
    "            print(f'=========================={descriptor_name:<20} R2 score: {r2_result:.4f} ({duration_time:.3f} sec)')\n",
    "\n",
    "        except Exception as inner_e:\n",
    "            print(f\"[{descriptor_name}] Error during learning process: {inner_e}\")\n",
    "            r2_list[descriptor_name] = 0.0\n",
    "            duration_time = time.time() - start\n",
    "            if esti_time is not None:\n",
    "                esti_time[descriptor_name] = duration_time\n",
    "            \n",
    "        finally:\n",
    "            # Clean up memory\n",
    "            del new_fps\n",
    "            gc.collect()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"[{descriptor_name}] Error in evaluate_descriptor: {e}\")\n",
    "        r2_list[descriptor_name] = 0.0\n",
    "        if esti_time is not None:\n",
    "            esti_time[descriptor_name] = time.time() - start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_descriptor(fps, descriptor, descriptor_name, y_true, r2_list, cpu_calc=False, esti_time=None):\n",
    "#     # try:\n",
    "#     new_fps = fps.copy()\n",
    "#     start = time.time()\n",
    "#     if descriptor is None:\n",
    "#         pass\n",
    "#     elif isinstance(descriptor, np.ndarray) and descriptor.ndim >= 2:\n",
    "#         # print(\"-1-\")\n",
    "#         # try:\n",
    "#         #     print(\"numpy\",descriptor.shape)\n",
    "#         # except:\n",
    "#         #     print(\"list\",len(descriptor))\n",
    "#         # print(descriptor)\n",
    "#         try:\n",
    "#             new_fps = np.concatenate([new_fps, descriptor], axis=1)\n",
    "#             del descriptor\n",
    "#         except Exception as e:\n",
    "#             print(f\"[-1-] Error occured: {e}\")\n",
    "#     elif isinstance(descriptor, list) and isinstance(descriptor[0], np.ndarray):\n",
    "#         # print(\"-2-\")\n",
    "#         # try:\n",
    "#         #     print(\"numpy\",descriptor.shape)\n",
    "#         # except:\n",
    "#         #     print(\"list\",len(descriptor))\n",
    "#         # print(descriptor)\n",
    "#         try:\n",
    "#             arrays_1d = [arr[:, None] for arr in descriptor if arr.ndim == 1]\n",
    "#             arrays_2d = [arr for arr in descriptor if arr.ndim == 2]\n",
    "#             combined_1d = np.concatenate(arrays_1d, axis=1) if arrays_1d else None\n",
    "#             combined_2d = np.concatenate(arrays_2d, axis=1) if arrays_2d else None\n",
    "#             to_concat = [new_fps] + [arr for arr in [combined_1d, combined_2d] if arr is not None]\n",
    "#             new_fps = np.concatenate(to_concat, axis=1)\n",
    "#             del descriptor, arrays_1d, arrays_2d\n",
    "#             if combined_1d is not None: del combined_1d\n",
    "#             if combined_2d is not None: del combined_2d\n",
    "#         except Exception as e:\n",
    "#             print(f\"[-2-] Error occured: {e}\")\n",
    "#     elif isinstance(descriptor, list) and isinstance(descriptor[0], list):\n",
    "#         # print(\"-3-\")\n",
    "#         # try:\n",
    "#         #     print(\"numpy\",descriptor.shape)\n",
    "#         # except:\n",
    "#         #     print(\"list\",len(descriptor))\n",
    "#         # print(descriptor)\n",
    "#         try:\n",
    "#             descriptor = np.asarray(descriptor).astype('float')\n",
    "#             new_fps = np.concatenate([new_fps, descriptor], axis=1)\n",
    "#             del descriptor\n",
    "#         except Exception as e:\n",
    "#             print(f\"[-3-] Error occured: {e}\")\n",
    "#     else:\n",
    "#         # print(\"-4-\")\n",
    "#         # try:\n",
    "#         #     print(\"numpy\",descriptor.shape)\n",
    "#         # except:\n",
    "#         #     print(\"list\",len(descriptor))\n",
    "#         # print(descriptor)\n",
    "#         descriptor = np.asarray(descriptor).astype('float')\n",
    "#         new_fps = np.concatenate([new_fps, descriptor[:,None]], axis=1)\n",
    "#         del descriptor\n",
    "            \n",
    "#         #######################################################\n",
    "#         #######################################################\n",
    "#         #######################################################\n",
    "#         #######################################################\n",
    "#     try:\n",
    "#         new_fps = np.nan_to_num(new_fps, nan=0.0, posinf=0.0, neginf=0.0).astype('float')\n",
    "#         y_true = np.asarray(y_true).astype('float')\n",
    "#         if cpu_calc:\n",
    "#             r2_list[descriptor_name] = learning_result(new_fps, y_true)\n",
    "#         else:\n",
    "#             np.save('new_fps.npy', new_fps)\n",
    "#             np.save('y_true.npy', y_true)\n",
    "            \n",
    "#             save_model(new_fps)  \n",
    "            \n",
    "#             result = subprocess.run(['python3', './extra_code/learning_process.py', \n",
    "#                                     str(BATCHSIZE), str(EPOCHS), \n",
    "#                                     str(lr),\n",
    "#                                     'new_fps.npy', 'y_true.npy'],\n",
    "#                                 stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "            \n",
    "#             if result.stderr:\n",
    "#                 filtered_stderr = '\\n'.join([line for line in result.stderr.split('\\n') if \"could not open file to read NUMA node\" not in line and \"Your kernel may have been built without NUMA support\" not in line])\n",
    "#                 if filtered_stderr:\n",
    "#                     print(f\"Error in subprocess: {filtered_stderr}\", file=sys.stderr)\n",
    "\n",
    "#             for line in result.stdout.splitlines():\n",
    "#                 if \"R2\" in line:\n",
    "#                     if \"(prune)\" in line:\n",
    "#                         print(f\"Pruning trial due to poor R2: {line}\")\n",
    "#                         r2_result = 0.0\n",
    "#                     else:\n",
    "#                         r2_result = float(line.split(\":\")[1].strip())\n",
    "#                         print(f\"R2 score: {r2_result}\")\n",
    "#             r2_list[descriptor_name] = r2_result\n",
    "\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error during learning result process: {e}\", file=sys.stderr)\n",
    "#         r2_list[descriptor_name] = 0.0\n",
    "\n",
    "#     duration_time = time.time() - start\n",
    "#     if esti_time is not None:\n",
    "#         esti_time[descriptor_name] = duration_time\n",
    "\n",
    "#     print(f'=========================={descriptor_name:<20} R2 score: {r2_result:.4f} ({duration_time:.3f} sec)')\n",
    "\n",
    "#     del new_fps\n",
    "#     gc.collect()\n",
    "        \n",
    "#     # except Exception as e:\n",
    "#     #     print(f\"Error in evaluate_descriptor: {e}, Descriptor: {descriptor_name}\", file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 241023\n",
    "# def evaluate_descriptor(fps, descriptor, descriptor_name, y_true, r2_list, esti_time=None):\n",
    "#     try:\n",
    "#         new_fps = fps.copy()\n",
    "#         start = time.time()\n",
    "#         if descriptor is None:\n",
    "#             pass\n",
    "#         elif isinstance(descriptor, np.ndarray) and len(descriptor.shape) == 2:\n",
    "#             if new_fps.shape[0] != descriptor.shape[0]:\n",
    "#                 raise ValueError(f\"-2- Shape mismatch: fps has {new_fps.shape[0]} rows, but descriptor has {descriptor.shape[0]} rows\")\n",
    "#             new_fps = np.concatenate([new_fps, descriptor], axis=1)\n",
    "#             del descriptor\n",
    "#         elif isinstance(descriptor, list) and isinstance(descriptor[0], np.ndarray):\n",
    "#             descriptor_2d = [arr.reshape(-1, 1) if arr.ndim == 1 else arr for arr in descriptor]\n",
    "#             descriptor_df = np.concatenate(descriptor_2d, axis=1)\n",
    "#             new_fps = np.concatenate([new_fps, descriptor_df], axis=1)\n",
    "#             del descriptor, descriptor_df, descriptor_2d\n",
    "#         elif isinstance(descriptor, list) and isinstance(descriptor[0], list):\n",
    "#             descriptor = np.asarray(descriptor)\n",
    "#             new_fps = np.concatenate((new_fps, descriptor), axis=1)\n",
    "#             del descriptor\n",
    "#         else:\n",
    "#             descriptor = np.asarray(descriptor).reshape(-1, 1)\n",
    "#             if new_fps.shape[0] != descriptor.shape[0]:\n",
    "#                 raise ValueError(f\"-3- Shape mismatch: fps has {new_fps.shape[0]} rows, but descriptor has {descriptor.shape[0]} rows\")\n",
    "#             new_fps = np.concatenate((new_fps, descriptor), axis=1)\n",
    "#             del descriptor\n",
    "            \n",
    "#         #######################################################\n",
    "#         #######################################################\n",
    "#         #######################################################\n",
    "#         #######################################################\n",
    "#         try:\n",
    "#             new_fps = np.nan_to_num(new_fps, nan=0.0, posinf=0.0, neginf=0.0).astype('float')\n",
    "#             y_true = np.asarray(y_true).astype('float')\n",
    "#             np.save('new_fps.npy', new_fps)\n",
    "#             np.save('y_true.npy', y_true)\n",
    "            \n",
    "#             save_model(new_fps)  \n",
    "            \n",
    "#             result = subprocess.run(['python3', './extra_code/learning_process.py', \n",
    "#                                  str(BATCHSIZE), str(EPOCHS), \n",
    "#                                  str(lr),\n",
    "#                                  'new_fps.npy', 'y_true.npy'],\n",
    "#                                 stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "            \n",
    "#             if result.stderr:\n",
    "#                 filtered_stderr = '\\n'.join([line for line in result.stderr.split('\\n') if \"could not open file to read NUMA node\" not in line and \"Your kernel may have been built without NUMA support\" not in line])\n",
    "#                 if filtered_stderr:\n",
    "#                     print(f\"Error in subprocess: {filtered_stderr}\", file=sys.stderr)\n",
    "\n",
    "#             for line in result.stdout.splitlines():\n",
    "#                 if \"R2\" in line:\n",
    "#                     if \"(prune)\" in line:\n",
    "#                         print(f\"Pruning trial due to poor R2: {line}\")\n",
    "#                         r2_result = 0.0\n",
    "#                     else:\n",
    "#                         r2_result = float(line.split(\":\")[1].strip())\n",
    "#                         print(f\"R2 score: {r2_result}\")\n",
    "\n",
    "#             # if result.returncode != 0:\n",
    "#             #     raise ValueError(f\"[{descriptor_name}] Error during learning result process: {result.stderr}\")\n",
    "#             # try:\n",
    "#             #     r2_result = float(result.stdout.strip())\n",
    "#             # except ValueError:\n",
    "#             #     raise ValueError(f\"[{descriptor_name}] Unable to parse R² score from output: {result.stdout}\")\n",
    "\n",
    "#             # if r2_result is None:\n",
    "#             #     raise ValueError(f\"[{descriptor_name}] R² score is None\")\n",
    "#             r2_list[descriptor_name] = r2_result\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error during learning result process: {e}\", file=sys.stderr)\n",
    "#             r2_list[descriptor_name] = 0.0\n",
    "\n",
    "#         duration_time = time.time() - start\n",
    "#         if esti_time is not None:\n",
    "#             esti_time[descriptor_name] = duration_time\n",
    "\n",
    "#         print(f'=========================={descriptor_name:<20} R2 score: {r2_result:.4f} ({duration_time:.3f} sec)')\n",
    "\n",
    "#         del new_fps\n",
    "#         gc.collect()\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in evaluate_descriptor: {e}, Descriptor: {descriptor_name}\", file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_descriptor(fps, descriptor, descriptor_name, y_true, r2_list, esti_time=None):\n",
    "#     try:\n",
    "#         new_fps = fps.copy()\n",
    "#         start = time.time()\n",
    "#         if descriptor is None:\n",
    "#             pass\n",
    "#         elif isinstance(descriptor, np.ndarray) and len(descriptor.shape) == 2:\n",
    "#             if new_fps.shape[0] != descriptor.shape[0]:\n",
    "#                 raise ValueError(f\"-2- Shape mismatch: fps has {new_fps.shape[0]} rows, but descriptor has {descriptor.shape[0]} rows\")\n",
    "#             new_fps = np.concatenate([new_fps, descriptor], axis=1)\n",
    "#             del descriptor\n",
    "#         elif isinstance(descriptor, list) and isinstance(descriptor[0], np.ndarray):\n",
    "#             descriptor_2d = [arr.reshape(-1, 1) if arr.ndim == 1 else arr for arr in descriptor]\n",
    "#             descriptor_df = np.concatenate(descriptor_2d, axis=1)\n",
    "#             new_fps = np.concatenate([new_fps, descriptor_df], axis=1)\n",
    "#             del descriptor, descriptor_df, descriptor_2d\n",
    "#         elif isinstance(descriptor, list) and isinstance(descriptor[0], list):\n",
    "#             descriptor = np.asarray(descriptor)\n",
    "#             new_fps = np.concatenate((new_fps, descriptor), axis=1)\n",
    "#             del descriptor\n",
    "#         else:\n",
    "#             descriptor = np.asarray(descriptor).reshape(-1, 1)\n",
    "#             if new_fps.shape[0] != descriptor.shape[0]:\n",
    "#                 raise ValueError(f\"-3- Shape mismatch: fps has {new_fps.shape[0]} rows, but descriptor has {descriptor.shape[0]} rows\")\n",
    "#             new_fps = np.concatenate((new_fps, descriptor), axis=1)\n",
    "#             del descriptor\n",
    "\n",
    "#         try:\n",
    "#             new_fps = np.nan_to_num(new_fps, nan=0.0, posinf=0.0, neginf=0.0).astype('float')\n",
    "#             y_true = np.asarray(y_true).astype('float')\n",
    "#             np.save('new_fps.npy', new_fps)\n",
    "#             np.save('y_true.npy', y_true)\n",
    "            \n",
    "#             save_model(new_fps)  \n",
    "            \n",
    "#             result = subprocess.run(['python3', './extra_code/learning_process.py', \n",
    "#                                  str(BATCHSIZE), str(EPOCHS), \n",
    "#                                  str(lr),\n",
    "#                                  'new_fps.npy', 'y_true.npy'],\n",
    "#                                 stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "            \n",
    "#             if result.stderr:\n",
    "#                 sys.stderr.write(result.stderr)\n",
    "\n",
    "#             if result.returncode != 0:\n",
    "#                 raise ValueError(f\"[{descriptor_name}] Error during learning result process: {result.stderr}\")\n",
    "\n",
    "#             try:\n",
    "#                 r2_result = float(result.stdout.strip())\n",
    "#             except ValueError:\n",
    "#                 raise ValueError(f\"[{descriptor_name}] Unable to parse R² score from output: {result.stdout}\")\n",
    "\n",
    "#             if r2_result is None:\n",
    "#                 raise ValueError(f\"[{descriptor_name}] R² score is None\")\n",
    "#             r2_list[descriptor_name] = r2_result\n",
    "            \n",
    "#         except Exception as inner_e:\n",
    "#             print(f\"Error during learning result process: {inner_e}\", file=sys.stderr)\n",
    "#             r2_list[descriptor_name] = 0\n",
    "\n",
    "#         duration_time = time.time() - start\n",
    "#         if esti_time is not None:\n",
    "#             esti_time[descriptor_name] = duration_time\n",
    "\n",
    "#         print(f'=========================={descriptor_name:<20} R2 score: {r2_result:.4f} ({duration_time:.3f} sec)')\n",
    "\n",
    "#         del new_fps\n",
    "#         gc.collect()\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in evaluate_descriptor: {e}, Descriptor: {descriptor_name}\", file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalization(descriptor):\n",
    "    descriptor = np.asarray(descriptor)\n",
    "    epsilon = 1e-10\n",
    "    max_value = 1e15\n",
    "    descriptor = np.clip(descriptor, -max_value, max_value)\n",
    "    descriptor_custom = np.where(np.abs(descriptor) < epsilon, epsilon, descriptor)\n",
    "    descriptor_log = np.sign(descriptor_custom) * np.log1p(np.abs(descriptor_custom))\n",
    "    descriptor_log = np.nan_to_num(descriptor_log, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    del epsilon\n",
    "    gc.collect()    \n",
    "    return descriptor_log\n",
    "\n",
    "# def Normalization(descriptor, logOnly=True):\n",
    "#     if logOnly:\n",
    "#         descriptor = np.asarray(descriptor)\n",
    "#         descriptor = np.log1p(descriptor+0.0001)\n",
    "#         descriptor = np.nan_to_num(descriptor, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "#         gc.collect()\n",
    "#         return descriptor\n",
    "#     else:\n",
    "#         epsilon = 1e-10\n",
    "#         descriptor_adjusted = np.where(np.abs(descriptor) < epsilon, epsilon, descriptor)\n",
    "#         descriptor_log_transformed = np.sign(descriptor_adjusted) * np.log1p(np.abs(descriptor_adjusted))\n",
    "#         del epsilon\n",
    "#         gc.collect()\n",
    "#         return descriptor_log_transformed\n",
    "    \n",
    "\n",
    "# # Original R2 score: 0.7853 (9.152 sec)\n",
    "# # Ipc R2 score: 0.8647 (8.066 sec)\n",
    "# # PMI1 R2 score: 0.8014 (8.812 sec)\n",
    "# # PMI2 R2 score: 0.7945 (8.344 sec)\n",
    "# # PMI3 R2 score: 0.7944 (8.918 sec)\n",
    "# # PMI_ALL_sum R2 score: 0.7639 (8.805 sec)\n",
    "# # PMI_ALL_ind R2 score: 0.7968 (9.276 sec)\n",
    "# # MORSE R2 score: 0.7654 (9.119 sec)\n",
    "# # GETAWAY R2 score: 0.7837 (8.395 sec)\n",
    "# # <class 'dict'> {'Original': 0.7853323221206665, 'Ipc': 0.8647328019142151, 'PMI1': 0.8014324903488159, 'PMI2': 0.7944905161857605, 'PMI3': 0.7943621873855591, 'PMI_ALL_sum': 0.763858437538147, 'PMI_ALL_ind': 0.7968299388885498, 'MORSE': 0.7653760313987732, 'GETAWAY': 0.7837035059928894\n",
    "\n",
    "# # 1. Min-Max 스케일링\n",
    "# def Normalization1(descriptor):\n",
    "#     min_val = np.min(descriptor)\n",
    "#     max_val = np.max(descriptor)\n",
    "#     return (descriptor - min_val) / (max_val - min_val)\n",
    "#         # 모든 값을 0과 1 사이로 변환합니다.\n",
    "#         # 원래 데이터의 상대적인 차이를 유지합니다.\n",
    "#         # 음수 값도 처리할 수 있습니다.\n",
    "#         # 단점: 이상치에 민감할 수 있습니다.\n",
    "\n",
    "# # Original R2 score: 0.7897 (9.397 sec)\n",
    "# # Ipc R2 score: 0.7891 (8.114 sec)\n",
    "# # PMI1 R2 score: 0.7897 (8.418 sec)\n",
    "# # PMI2 R2 score: 0.7937 (8.643 sec)\n",
    "# # PMI3 R2 score: 0.7960 (8.784 sec)\n",
    "# # PMI_ALL_sum R2 score: 0.8045 (8.100 sec)\n",
    "# # PMI_ALL_ind R2 score: 0.7887 (8.632 sec)\n",
    "# # MORSE R2 score: 0.8002 (8.797 sec)\n",
    "# # GETAWAY R2 score: 0.7991 (8.633 sec)\n",
    "# # <class 'dict'> {'Original': 0.7897382974624634, 'Ipc': 0.7890591621398926, 'PMI1': 0.7897088527679443, 'PMI2': 0.7937353849411011, 'PMI3': 0.795976459980011, 'PMI_ALL_sum': 0.8044917583465576, 'PMI_ALL_ind': 0.7886590957641602, 'MORSE': 0.8001644611358643, 'GETAWAY': 0.7991337776184082        \n",
    "\n",
    "\n",
    "# # 2. 로버스트 스케일링\n",
    "# def Normalization2(descriptor):\n",
    "#     median = np.median(descriptor)\n",
    "#     q1, q3 = np.percentile(descriptor, [25, 75])\n",
    "#     iqr = q3 - q1\n",
    "#     return (descriptor - median) / iqr\n",
    "#         # 중앙값을 0으로, 사분위수 범위를 1로 만듭니다.\n",
    "#         # 이상치의 영향을 줄입니다.\n",
    "#         # 음수와 양수 값을 모두 유지합니다. \n",
    "\n",
    "# # Original R2 score: 0.7555 (8.747 sec)\n",
    "# # Ipc R2 score: 0.7010 (8.247 sec)\n",
    "# # PMI1 R2 score: 0.8046 (8.603 sec)\n",
    "# # PMI2 R2 score: 0.7892 (8.903 sec)\n",
    "# # PMI3 R2 score: 0.7925 (8.180 sec)\n",
    "# # PMI_ALL_sum R2 score: 0.8078 (8.460 sec)\n",
    "# # PMI_ALL_ind R2 score: 0.8207 (8.786 sec)\n",
    "# # MORSE R2 score: -0.4633 (8.368 sec)\n",
    "# # GETAWAY R2 score: 0.7914 (8.156 sec)\n",
    "# # <class 'dict'> {'Original': 0.7555391192436218, 'Ipc': 0.7009564638137817, 'PMI1': 0.804631233215332, 'PMI2': 0.7892019748687744, 'PMI3': 0.7925474643707275, 'PMI_ALL_sum': 0.8078181147575378, 'PMI_ALL_ind': 0.820658266544342, 'MORSE': -0.46331870555877686, 'GETAWAY': 0.7913508415222168\n",
    "\n",
    "\n",
    "# # 3. 표준화 (Z-score 정규화):\n",
    "# def Normalization3(descriptor):\n",
    "#     mean = np.mean(descriptor)\n",
    "#     std = np.std(descriptor)\n",
    "#     return (descriptor - mean) / std\n",
    "#         # 평균을 0으로, 표준편차를 1로 만듭니다.\n",
    "#         # 음수와 양수 값을 모두 유지합니다.\n",
    "#         # 정규 분포를 따르는 데이터에 적합합니다.\n",
    "\n",
    "# Original R2 score: 0.7848 (8.133 sec)\n",
    "# WARNING:tensorflow:5 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f23178bfe20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
    "# WARNING:tensorflow:6 out of the last 12 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f23178bfe20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
    "# Ipc R2 score: 0.7911 (8.218 sec)\n",
    "# PMI1 R2 score: 0.7916 (7.400 sec)\n",
    "# PMI2 R2 score: 0.8127 (8.217 sec)\n",
    "# PMI3 R2 score: 0.7969 (8.257 sec)\n",
    "# PMI_ALL_sum R2 score: 0.8016 (8.246 sec)\n",
    "# PMI_ALL_ind R2 score: 0.8103 (7.652 sec)\n",
    "# MORSE R2 score: 0.8166 (8.539 sec)\n",
    "# GETAWAY R2 score: 0.8039 (8.539 sec)\n",
    "# <class 'dict'> {'Original': 0.784812867641449, 'Ipc': 0.7910993099212646, 'PMI1': 0.7916101813316345, 'PMI2': 0.8127365708351135, 'PMI3': 0.7969383001327515, 'PMI_ALL_sum': 0.8016161918640137, 'PMI_ALL_ind': 0.8103429079055786, 'MORSE': 0.8166193962097168, 'GETAWAY': 0.8039106130599976}\n",
    "\n",
    "# # 4. 중앙값과 절대편차 (MAD) 정규화:\n",
    "# def Normalization4(descriptor):\n",
    "#     median = np.median(descriptor)\n",
    "#     mad = np.median(np.abs(descriptor - median))\n",
    "#     return (descriptor - median) / mad\n",
    "#         # 중앙값을 0으로, MAD를 1로 만듭니다.\n",
    "#         # 이상치에 매우 강건합니다.\n",
    "#         # 음수와 양수 값을 모두 유지합니다.\n",
    "\n",
    "# # Original R2 score: 0.7931 (10.758 sec)\n",
    "# # Ipc R2 score: -1150.3533 (8.209 sec)\n",
    "# # PMI1 R2 score: 0.7592 (8.801 sec)\n",
    "# # PMI2 R2 score: 0.7604 (9.580 sec)\n",
    "# # PMI3 R2 score: 0.7664 (8.281 sec)\n",
    "# # PMI_ALL_sum R2 score: 0.7016 (9.358 sec)\n",
    "# # PMI_ALL_ind R2 score: 0.7478 (9.451 sec)\n",
    "# # MORSE R2 score: -1.5698 (8.654 sec)\n",
    "# # GETAWAY R2 score: 0.7888 (8.044 sec)\n",
    "# # <class 'dict'> {'Original': 0.7931280732154846, 'Ipc': -1150.353271484375, 'PMI1': 0.7591773271560669, 'PMI2': 0.7603589296340942, 'PMI3': 0.7663605213165283, 'PMI_ALL_sum': 0.7015960812568665, 'PMI_ALL_ind': 0.7478456497192383, 'MORSE': -1.56978440284729, 'GETAWAY': 0.7888303995132446\n",
    "\n",
    "# # 5. 최대 절대값 스케일링\n",
    "# def Normalization5(descriptor):\n",
    "#     max_abs = np.max(np.abs(descriptor))\n",
    "#     return descriptor / max_abs\n",
    "#         # 모든 값을 -1과 1 사이로 변환합니다.\n",
    "#         # 0을 중심으로 하는 데이터에 유용합니다.\n",
    "#         # 원래 데이터의 부호를 유지합니다.\n",
    "        \n",
    "# # Original R2 score: 0.7959 (9.297 sec)\n",
    "# # Ipc R2 score: 0.7932 (8.758 sec)\n",
    "# # PMI1 R2 score: 0.7897 (8.806 sec)\n",
    "# # PMI2 R2 score: 0.8081 (8.967 sec)\n",
    "# # PMI3 R2 score: 0.8110 (9.134 sec)\n",
    "# # PMI_ALL_sum R2 score: 0.7716 (8.160 sec)\n",
    "# # PMI_ALL_ind R2 score: 0.7780 (9.306 sec)\n",
    "# # MORSE R2 score: 0.7969 (8.748 sec)\n",
    "# # GETAWAY R2 score: 0.8007 (8.906 sec)\n",
    "# # <class 'dict'> {'Original': 0.7958791255950928, 'Ipc': 0.7931562066078186, 'PMI1': 0.7896729111671448, 'PMI2': 0.8081350922584534, 'PMI3': 0.8109619617462158, 'PMI_ALL_sum': 0.7715592384338379, 'PMI_ALL_ind': 0.7780115008354187, 'MORSE': 0.7968926429748535, 'GETAWAY': 0.8007307052612305}\n",
    "\n",
    "# # 6. Outlier\n",
    "# def Normalization6(descriptor, clip_threshold=3.0):\n",
    "#     # 1. 이상치 제거\n",
    "#     mean = np.mean(descriptor, axis=0)\n",
    "#     std = np.std(descriptor, axis=0)\n",
    "#     z_scores = np.abs((descriptor - mean) / std)\n",
    "#     descriptor_clipped = np.clip(descriptor, \n",
    "#                                  mean - clip_threshold * std, \n",
    "#                                  mean + clip_threshold * std)\n",
    "#     median = np.median(descriptor_clipped, axis=0)\n",
    "#     q1 = np.percentile(descriptor_clipped, 25, axis=0)\n",
    "#     q3 = np.percentile(descriptor_clipped, 75, axis=0)\n",
    "#     iqr = q3 - q1\n",
    "#     iqr = np.where(iqr == 0, 1e-6, iqr)    \n",
    "#     scaled_descriptor = (descriptor_clipped - median) / iqr\n",
    "#     final_descriptor = np.clip(scaled_descriptor, -5, 5)\n",
    "    \n",
    "#     return final_descriptor\n",
    "\n",
    "# # Original R2 score: 0.7818 (9.309 sec)\n",
    "# # Ipc R2 score: 0.8058 (8.277 sec)\n",
    "# # PMI1 R2 score: 0.8024 (9.047 sec)\n",
    "# # PMI2 R2 score: 0.7887 (8.503 sec)\n",
    "# # PMI3 R2 score: 0.7969 (9.133 sec)\n",
    "# # PMI_ALL_sum R2 score: 0.8020 (9.083 sec)\n",
    "# # PMI_ALL_ind R2 score: 0.8004 (9.157 sec)\n",
    "# # MORSE R2 score: 0.6951 (8.468 sec)\n",
    "# # GETAWAY R2 score: 0.6884 (8.690 sec)\n",
    "# # <class 'dict'> {'Original': 0.7818286418914795, 'Ipc': 0.8058270812034607, 'PMI1': 0.8023815155029297, 'PMI2': 0.7886528372764587, 'PMI3': 0.7969330549240112, 'PMI_ALL_sum': 0.8020067811012268, 'PMI_ALL_ind': 0.8003607392311096, 'MORSE': 0.6950668096542358, 'GETAWAY': 0.6883505582809448}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def values_chi(mol, chi_type):\n",
    "    i = 0\n",
    "    chi_func = Chem.GraphDescriptors.ChiNn_ if chi_type == 'n' else Chem.GraphDescriptors.ChiNv_\n",
    "    while chi_func(mol, i) != 0.0:\n",
    "        i += 1\n",
    "    return np.array([chi_func(mol, j) for j in range(i)])\n",
    "\n",
    "def generate_chi(mols, chi_type, n_jobs=None):\n",
    "    with ProcessPoolExecutor(max_workers=n_jobs) as executor:\n",
    "        futures = [executor.submit(values_chi, mol, chi_type) for mol in mols]\n",
    "        descriptor = [future.result() for future in futures]\n",
    "    \n",
    "    max_length = max(len(x) for x in descriptor)\n",
    "    padded_descriptor = np.array([np.pad(x, (0, max_length - len(x)), 'constant') for x in descriptor])\n",
    "    \n",
    "    return padded_descriptor\n",
    "\n",
    "def sanitize_and_compute_descriptor(mol):\n",
    "    try:\n",
    "        mol = Chem.RemoveHs(mol)        \n",
    "        Chem.SanitizeMol(mol)        \n",
    "        try:\n",
    "            return Chem.rdMolDescriptors.BCUT2D(mol)\n",
    "        except Exception as e:\n",
    "            print(f\"BCUT2D calculation failed: {e}\")            \n",
    "            # Fallback to a simpler descriptor (e.g., MolWt)\n",
    "            return [Descriptors.MolWt(mol)] * 8\n",
    "    except Exception as e:\n",
    "        # smiles = Chem.MolToSmiles(mol) if mol else 'Unknown'\n",
    "        # print(f\"Error with molecule: {smiles} - {e}\")\n",
    "        return [0] * 8\n",
    "\n",
    "def compute_descriptors_parallel(mols, n_jobs=None):\n",
    "    with ProcessPoolExecutor(max_workers=n_jobs) as executor:\n",
    "        futures = [executor.submit(sanitize_and_compute_descriptor, mol) for mol in mols if mol is not None]\n",
    "        descriptors = [future.result() for future in futures]\n",
    "    return np.array(descriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_molecules_parallel(mols, max_workers=4, chunk_size=100):\n",
    "    results = []    \n",
    "    for i in range(0, len(mols), chunk_size):\n",
    "        chunk = mols[i:i + chunk_size]        \n",
    "        with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = [executor.submit(mol3d, mol) for mol in chunk]\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                if result is not None:\n",
    "                    results.append(result)        \n",
    "        gc.collect()    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_resources(local_vars):\n",
    "    for name, _ in list(local_vars.items()):\n",
    "        if name not in ['r2_list', 'esti_time']:  # Keep the return value\n",
    "            del local_vars[name]\n",
    "    devices = tf.config.list_logical_devices('GPU')\n",
    "    if devices:\n",
    "        with tf.device(devices[0].name):\n",
    "            tf.keras.backend.clear_session()\n",
    "            gc.collect()        \n",
    "    gc.collect()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptors_list(fps, mols, y_true, target_name=None, target_path=\"\", cpu_calc=False, read_logs=False):\n",
    "    r2_list={}\n",
    "    esti_time = {}\n",
    "    #########################################\n",
    "    try:\n",
    "        evaluate_descriptor(fps, None, 'Original', y_true, r2_list, esti_time, cpu_calc)\n",
    "        gc.collect()\n",
    "        descriptor = [Chem.Descriptors.ExactMolWt(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor, 'ExactMolWt', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del descriptor\n",
    "        gc.collect()\n",
    "        descriptor = [Chem.Descriptors.MolWt(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor, 'MolWt', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del descriptor\n",
    "        gc.collect()\n",
    "        descriptor = [Chem.Crippen.MolLogP(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor, 'MolLogP', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del descriptor\n",
    "        gc.collect()\n",
    "        descriptor = [Chem.Crippen.MolMR(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor, 'MolMR', y_true, r2_list, esti_time, cpu_calc) \n",
    "        del descriptor\n",
    "        gc.collect()\n",
    "        descriptor = [Chem.Descriptors.TPSA(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor, 'TPSA', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del descriptor\n",
    "        gc.collect()\n",
    "        descriptor = [Chem.Lipinski.NumRotatableBonds(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor, 'NumRotatableBonds', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del descriptor\n",
    "        gc.collect()\n",
    "        descriptor = [Chem.Lipinski.HeavyAtomCount(alpha)     for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor, 'HeavyAtomCount', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del descriptor\n",
    "        gc.collect()\n",
    "        descriptor = [Chem.Lipinski.NumHAcceptors(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor, 'NumHAcceptors', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del descriptor\n",
    "        gc.collect()\n",
    "        descriptor = [Chem.Lipinski.NumHDonors(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor, 'NumHDonors', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del descriptor\n",
    "        gc.collect()\n",
    "        descriptor = [Chem.Lipinski.NumHeteroatoms(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor, 'NumHeteroatoms', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del descriptor\n",
    "        gc.collect()\n",
    "        descriptor = [Chem.Lipinski.NHOHCount(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor, 'NHOHCount', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del descriptor\n",
    "        gc.collect()\n",
    "        descriptor = [Chem.Lipinski.NOCount(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor, 'NOCount', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del descriptor\n",
    "        gc.collect()\n",
    "        descriptor = [Chem.Lipinski.RingCount(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor, 'RingCount', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del descriptor\n",
    "        gc.collect()\n",
    "        descriptor = [Chem.Lipinski.NumAromaticRings(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor, 'NumAromaticRings', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del descriptor\n",
    "        gc.collect()\n",
    "        descriptor = [Chem.Lipinski.NumSaturatedRings(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor, 'NumSaturatedRings', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del descriptor\n",
    "        gc.collect()\n",
    "        descriptor = [Chem.Lipinski.NumAliphaticRings(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor, 'NumAliphaticRings', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del descriptor\n",
    "        gc.collect()\n",
    "        descriptor = [Chem.rdMolDescriptors.CalcLabuteASA(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor, 'CalcLabuteASA', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del descriptor\n",
    "        gc.collect()\n",
    "        descriptor = [Chem.Descriptors.NumValenceElectrons(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor, 'NumValenceElectrons', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del descriptor\n",
    "        gc.collect()\n",
    "        descriptor = [Chem.GraphDescriptors.BalabanJ(alpha) for alpha in mols]\n",
    "        # descriptor = Normalization(descriptor)\n",
    "        evaluate_descriptor(fps, descriptor, 'BalabanJ', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del descriptor\n",
    "        gc.collect()\n",
    "        descriptor = [Chem.GraphDescriptors.BertzCT(alpha) for alpha in mols]\n",
    "        # descriptor = Normalization(descriptor)\n",
    "        evaluate_descriptor(fps, descriptor, 'BertzCT', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del descriptor\n",
    "        gc.collect()\n",
    "        descriptor = [Chem.GraphDescriptors.Ipc(alpha) for alpha in mols]\n",
    "        descriptor = Normalization(descriptor)\n",
    "        evaluate_descriptor(fps, descriptor, 'Ipc', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del descriptor\n",
    "        gc.collect()\n",
    "        descriptor1 = [Chem.GraphDescriptors.Kappa1(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor1, 'Kappa1', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor2 = [Chem.GraphDescriptors.Kappa2(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor2, 'Kappa2', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor3 = [Chem.GraphDescriptors.Kappa3(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor3, 'Kappa3', y_true, r2_list, esti_time, cpu_calc)\n",
    "        d1 = np.asarray(descriptor1).astype('float')\n",
    "        d2 = np.asarray(descriptor2).astype('float')\n",
    "        d3 = np.asarray(descriptor3).astype('float')\n",
    "        del descriptor1,descriptor2,descriptor3\n",
    "        gc.collect()\n",
    "        evaluate_descriptor(fps, d1+d2+d3, 'Kappa_all_sum', y_true, r2_list, esti_time, cpu_calc)\n",
    "        dataset = [d1, d2, d3]\n",
    "        evaluate_descriptor(fps, dataset, 'Kappa_all_ind', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del dataset,d1, d2, d3\n",
    "        #########################################\n",
    "        if read_logs:\n",
    "            print(\"# 1 Finished\")\n",
    "        #########################################\n",
    "        descriptor1 = [Chem.GraphDescriptors.Chi0(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor1, 'Chi0', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor2 = [Chem.GraphDescriptors.Chi0n(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor2, 'Chi0n', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor3 = [Chem.GraphDescriptors.Chi0v(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor3, 'Chi0v', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor4 = [Chem.GraphDescriptors.Chi1(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor4, 'Chi1', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor5 = [Chem.GraphDescriptors.Chi1n(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor5, 'Chi1n', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor6 = [Chem.GraphDescriptors.Chi1v(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor6, 'Chi1v', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor7 = [Chem.GraphDescriptors.Chi2n(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor7, 'Chi2n', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor8 = [Chem.GraphDescriptors.Chi2v(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor8, 'Chi2v', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor9 = [Chem.GraphDescriptors.Chi3n(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor9, 'Chi3n', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor10 = [Chem.GraphDescriptors.Chi3v(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor10, 'Chi3v', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor11 = [Chem.GraphDescriptors.Chi4n(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor11, 'Chi4n', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor12 = [Chem.GraphDescriptors.Chi4v(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor12, 'Chi4v', y_true, r2_list, esti_time, cpu_calc)\n",
    "        #########################################\n",
    "        descriptor13 = generate_chi(mols, 'n')\n",
    "        evaluate_descriptor(fps, descriptor13, 'ChiN_', y_true, r2_list, esti_time, cpu_calc)\n",
    "        #########################################\n",
    "        descriptor14 = generate_chi(mols, 'v')\n",
    "        evaluate_descriptor(fps, descriptor14, 'ChiNv_', y_true, r2_list, esti_time, cpu_calc)\n",
    "        #########################################\n",
    "        d1  = np.asarray(descriptor1).astype('float')\n",
    "        d2  = np.asarray(descriptor2).astype('float')\n",
    "        d3  = np.asarray(descriptor3).astype('float')\n",
    "        d4  = np.asarray(descriptor4).astype('float')\n",
    "        d5  = np.asarray(descriptor5).astype('float')\n",
    "        d6  = np.asarray(descriptor6).astype('float')\n",
    "        d7  = np.asarray(descriptor7).astype('float')\n",
    "        d8  = np.asarray(descriptor8).astype('float')\n",
    "        d9  = np.asarray(descriptor9).astype('float')\n",
    "        d10 = np.asarray(descriptor10).astype('float')\n",
    "        d11 = np.asarray(descriptor11).astype('float')\n",
    "        d12 = np.asarray(descriptor12).astype('float')\n",
    "        d13 = np.asarray(descriptor13).astype('float')\n",
    "        d14 = np.asarray(descriptor14).astype('float')\n",
    "        td13 = d13.mean(axis=1)\n",
    "        td14 = d14.mean(axis=1)\n",
    "        evaluate_descriptor(fps, d1+d2+d3+d4+d5+d6+d7+d8+d9+d10+d11+d12+td13+td14, 'Chi_ALL_sum', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del descriptor1,descriptor2,descriptor3,descriptor4,descriptor5,descriptor6,descriptor7,descriptor8,descriptor9,descriptor10,descriptor11,descriptor12, descriptor13, descriptor14\n",
    "        gc.collect()\n",
    "        dataset = [d1,d2,d3,d4,d5,d6,d7,d8,d9,d10,d11,d12,d13,d14]\n",
    "        evaluate_descriptor(fps, dataset, 'Chi_ALL_ind', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del dataset,d1,d2,d3,d4,d5,d6,d7,d8,d9,d10,d11,d12,d13,d14\n",
    "        ########################################\n",
    "        if read_logs:\n",
    "            print(\"# 2 - Chi_series Finished\")\n",
    "        #########################################\n",
    "        descriptor = [Chem.rdMolDescriptors.CalcPhi(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor, 'CalcPhi', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del descriptor\n",
    "        gc.collect()\n",
    "        descriptor = [Chem.GraphDescriptors.HallKierAlpha(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor, 'HallKierAlpha', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del descriptor\n",
    "        gc.collect()\n",
    "        descriptor = [Chem.rdMolDescriptors.CalcNumAmideBonds(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor, 'CalcNumAmideBonds', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del descriptor\n",
    "        gc.collect()\n",
    "        descriptor = [Chem.Lipinski.FractionCSP3(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor, 'FractionCSP3', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del descriptor\n",
    "        gc.collect()\n",
    "        descriptor = [Chem.rdMolDescriptors.CalcNumSpiroAtoms(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor, 'CalcNumSpiroAtoms', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del descriptor\n",
    "        gc.collect()\n",
    "        descriptor = [Chem.rdMolDescriptors.CalcNumBridgeheadAtoms(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor, 'CalcNumBridgeheadAtoms', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del descriptor\n",
    "        gc.collect()\n",
    "        #########################################\n",
    "        if read_logs:\n",
    "            print(\"# 3 Finished\")\n",
    "        #########################################\n",
    "        descriptor1 = [Chem.MolSurf.PEOE_VSA1(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor1, 'PEOE_VSA1', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor2 = [Chem.MolSurf.PEOE_VSA2(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor2, 'PEOE_VSA2', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor3 = [Chem.MolSurf.PEOE_VSA3(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor3, 'PEOE_VSA3', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor4 = [Chem.MolSurf.PEOE_VSA4(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor4, 'PEOE_VSA4', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor5 = [Chem.MolSurf.PEOE_VSA5(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor5, 'PEOE_VSA5', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor6 = [Chem.MolSurf.PEOE_VSA6(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor6, 'PEOE_VSA6', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor7 = [Chem.MolSurf.PEOE_VSA7(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor7, 'PEOE_VSA7', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor8 = [Chem.MolSurf.PEOE_VSA8(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor8, 'PEOE_VSA8', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor9 = [Chem.MolSurf.PEOE_VSA9(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor9, 'PEOE_VSA9', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor10 = [Chem.MolSurf.PEOE_VSA10(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor10, 'PEOE_VSA10', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor11 = [Chem.MolSurf.PEOE_VSA11(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor11, 'PEOE_VSA11', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor12 = [Chem.MolSurf.PEOE_VSA12(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor12, 'PEOE_VSA12', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor13 = [Chem.MolSurf.PEOE_VSA13(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor13, 'PEOE_VSA13', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor14 = [Chem.MolSurf.PEOE_VSA14(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor14, 'PEOE_VSA14', y_true, r2_list, esti_time, cpu_calc)\n",
    "        d1  = np.asarray(descriptor1 ).astype('float')\n",
    "        d2  = np.asarray(descriptor2 ).astype('float')\n",
    "        d3  = np.asarray(descriptor3 ).astype('float')\n",
    "        d4  = np.asarray(descriptor4 ).astype('float')\n",
    "        d5  = np.asarray(descriptor5 ).astype('float')\n",
    "        d6  = np.asarray(descriptor6 ).astype('float')\n",
    "        d7  = np.asarray(descriptor7 ).astype('float')\n",
    "        d8  = np.asarray(descriptor8 ).astype('float')\n",
    "        d9  = np.asarray(descriptor9 ).astype('float')\n",
    "        d10 = np.asarray(descriptor10).astype('float')\n",
    "        d11 = np.asarray(descriptor11).astype('float')\n",
    "        d12 = np.asarray(descriptor12).astype('float')\n",
    "        d13 = np.asarray(descriptor13).astype('float')\n",
    "        d14 = np.asarray(descriptor14).astype('float')\n",
    "        evaluate_descriptor(fps, d1+d2+d3+d4+d5+d6+d7+d8+d9+d10+d11+d12+d13+d14, 'PEOE_VSA_ALL_sum', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del descriptor1,descriptor2,descriptor3,descriptor4,descriptor5,descriptor6,descriptor7,descriptor8,descriptor9,descriptor10,descriptor11,descriptor12,descriptor13,descriptor14\n",
    "        gc.collect() \n",
    "        dataset = [d1,d2,d3,d4,d5,d6,d7,d8,d9,d10,d11,d12,d13,d14]\n",
    "        evaluate_descriptor(fps, dataset, 'PEOE_VSA_ALL_ind', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del dataset,d1,d2,d3,d4,d5,d6,d7,d8,d9,d10,d11,d12,d13,d14\n",
    "        #########################################\n",
    "        if read_logs:\n",
    "            print(\"# 4 - PEOE_VSA series Finished\")\n",
    "        #########################################\n",
    "        descriptor1 = [Chem.MolSurf.SMR_VSA1(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor1, 'SMR_VSA1', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor2 = [Chem.MolSurf.SMR_VSA2(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor2, 'SMR_VSA2', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor3 = [Chem.MolSurf.SMR_VSA3(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor3, 'SMR_VSA3', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor4 = [Chem.MolSurf.SMR_VSA4(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor4, 'SMR_VSA4', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor5 = [Chem.MolSurf.SMR_VSA5(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor5, 'SMR_VSA5', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor6 = [Chem.MolSurf.SMR_VSA6(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor6, 'SMR_VSA6', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor7 = [Chem.MolSurf.SMR_VSA7(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor7, 'SMR_VSA7', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor8 = [Chem.MolSurf.SMR_VSA8(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor8, 'SMR_VSA8', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor9 = [Chem.MolSurf.SMR_VSA9(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor9, 'SMR_VSA9', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor10 = [Chem.MolSurf.SMR_VSA10(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor10, 'SMR_VSA10', y_true, r2_list, esti_time, cpu_calc)\n",
    "        d1  = np.asarray(descriptor1 ).astype('float')\n",
    "        d2  = np.asarray(descriptor2 ).astype('float')\n",
    "        d3  = np.asarray(descriptor3 ).astype('float')\n",
    "        d4  = np.asarray(descriptor4 ).astype('float')\n",
    "        d5  = np.asarray(descriptor5 ).astype('float')\n",
    "        d6  = np.asarray(descriptor6 ).astype('float')\n",
    "        d7  = np.asarray(descriptor7 ).astype('float')\n",
    "        d8  = np.asarray(descriptor8 ).astype('float')\n",
    "        d9  = np.asarray(descriptor9 ).astype('float')\n",
    "        d10 = np.asarray(descriptor10).astype('float')\n",
    "        evaluate_descriptor(fps, d1+d2+d3+d4+d5+d6+d7+d8+d9+d10, 'SMR_VSA_ALL_SUM', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del descriptor1,descriptor2,descriptor3,descriptor4,descriptor5,descriptor6,descriptor7,descriptor8,descriptor9,descriptor10\n",
    "        gc.collect()\n",
    "        dataset = [d1,d2,d3,d4,d5,d6,d7,d8,d9,d10]\n",
    "        evaluate_descriptor(fps, dataset, 'SMR_VSA_ALL_IND', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del dataset,d1,d2,d3,d4,d5,d6,d7,d8,d9,d10\n",
    "        #########################################\n",
    "        if read_logs:\n",
    "            print(\"# 5 - SMR_VSA series Finished\")\n",
    "        #########################################\n",
    "        descriptor1 = [Chem.MolSurf.SlogP_VSA1(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor1, 'SlogP_VSA1', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor2 = [Chem.MolSurf.SlogP_VSA2(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor2, 'SlogP_VSA2', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor3 = [Chem.MolSurf.SlogP_VSA3(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor3, 'SlogP_VSA3', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor4 = [Chem.MolSurf.SlogP_VSA4(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor4, 'SlogP_VSA4', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor5 = [Chem.MolSurf.SlogP_VSA5(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor5, 'SlogP_VSA5', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor6 = [Chem.MolSurf.SlogP_VSA6(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor6, 'SlogP_VSA6', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor7 = [Chem.MolSurf.SlogP_VSA7(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor7, 'SlogP_VSA7', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor8 = [Chem.MolSurf.SlogP_VSA8(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor8, 'SlogP_VSA8', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor9 = [Chem.MolSurf.SlogP_VSA9(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor9, 'SlogP_VSA9', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor10 = [Chem.MolSurf.SlogP_VSA10(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor10, 'SlogP_VSA10', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor11 = [Chem.MolSurf.SlogP_VSA11(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor11, 'SlogP_VSA11', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor12 = [Chem.MolSurf.SlogP_VSA12(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor12, 'SlogP_VSA12', y_true, r2_list, esti_time, cpu_calc)\n",
    "        d1  = np.asarray(descriptor1 ).astype('float')\n",
    "        d2  = np.asarray(descriptor2 ).astype('float')\n",
    "        d3  = np.asarray(descriptor3 ).astype('float')\n",
    "        d4  = np.asarray(descriptor4 ).astype('float')\n",
    "        d5  = np.asarray(descriptor5 ).astype('float')\n",
    "        d6  = np.asarray(descriptor6 ).astype('float')\n",
    "        d7  = np.asarray(descriptor7 ).astype('float')\n",
    "        d8  = np.asarray(descriptor8 ).astype('float')\n",
    "        d9  = np.asarray(descriptor9 ).astype('float')\n",
    "        d10 = np.asarray(descriptor10).astype('float')\n",
    "        d11 = np.asarray(descriptor11).astype('float')\n",
    "        d12 = np.asarray(descriptor12).astype('float')\n",
    "        evaluate_descriptor(fps, d1+d2+d3+d4+d5+d6+d7+d8+d9+d10+d11+d12, 'SlogP_VSA_ALL_sum', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del descriptor1,descriptor2,descriptor3,descriptor4,descriptor5,descriptor6,descriptor7,descriptor8,descriptor9,descriptor10,descriptor11,descriptor12\n",
    "        gc.collect()\n",
    "        dataset = [d1,d2,d3,d4,d5,d6,d7,d8,d9,d10,d11,d12]\n",
    "        evaluate_descriptor(fps, dataset, 'SlogP_VSA_ALL_ind', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del dataset,d1,d2,d3,d4,d5,d6,d7,d8,d9,d10,d11,d12\n",
    "        #########################################\n",
    "        if read_logs:\n",
    "            print(\"# 6 - SlogP_VSA series Finished\")\n",
    "        #########################################\n",
    "        descriptor1 = [Chem.EState.EState_VSA.VSA_EState1(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor1, 'EState_VSA_EState1', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor2 = [Chem.EState.EState_VSA.VSA_EState2(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor2, 'EState_VSA_EState2', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor3 = [Chem.EState.EState_VSA.VSA_EState3(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor3, 'EState_VSA_EState3', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor4 = [Chem.EState.EState_VSA.VSA_EState4(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor4, 'EState_VSA_EState4', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor5 = [Chem.EState.EState_VSA.VSA_EState5(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor5, 'EState_VSA_EState5', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor6 = [Chem.EState.EState_VSA.VSA_EState6(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor6, 'EState_VSA_EState6', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor7 = [Chem.EState.EState_VSA.VSA_EState7(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor7, 'EState_VSA_EState7', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor8 = [Chem.EState.EState_VSA.VSA_EState8(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor8, 'EState_VSA_EState8', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor9 = [Chem.EState.EState_VSA.VSA_EState9(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor9, 'EState_VSA_EState9', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor10 = [Chem.EState.EState_VSA.VSA_EState10(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor10, 'EState_VSA_EState10', y_true, r2_list, esti_time, cpu_calc)\n",
    "        d1  = np.asarray(descriptor1 ).astype('float')\n",
    "        d2  = np.asarray(descriptor2 ).astype('float')\n",
    "        d3  = np.asarray(descriptor3 ).astype('float')\n",
    "        d4  = np.asarray(descriptor4 ).astype('float')\n",
    "        d5  = np.asarray(descriptor5 ).astype('float')\n",
    "        d6  = np.asarray(descriptor6 ).astype('float')\n",
    "        d7  = np.asarray(descriptor7 ).astype('float')\n",
    "        d8  = np.asarray(descriptor8 ).astype('float')\n",
    "        d9  = np.asarray(descriptor9 ).astype('float')\n",
    "        d10 = np.asarray(descriptor10).astype('float')\n",
    "        del descriptor1,descriptor2,descriptor3,descriptor4,descriptor5,descriptor6,descriptor7,descriptor8,descriptor9,descriptor10\n",
    "        gc.collect()\n",
    "        evaluate_descriptor(fps, d1+d2+d3+d4+d5+d6+d7+d8+d9+d10, 'VSA_EState_ALL_sum', y_true, r2_list, esti_time, cpu_calc)\n",
    "        dataset = [d1,d2,d3,d4,d5,d6,d7,d8,d9,d10]\n",
    "        evaluate_descriptor(fps, dataset, 'VSA_EState_ALL_ind', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del dataset,d1,d2,d3,d4,d5,d6,d7,d8,d9,d10\n",
    "        #########################################\n",
    "        if read_logs:\n",
    "            print(\"# 7 - VSA_EState series Finished\")\n",
    "            print(\"# 2D descriptors finished\")\n",
    "        ######################################### 3D Descriptors\n",
    "        start = time.time()\n",
    "        # mols2=[mol3d(mols) for mols in mols]\n",
    "        mols2 = process_molecules_parallel(mols, max_workers=8)\n",
    "        print(f\"Converted to mols2: {time.time()-start:.2f} sec\")\n",
    "        gc.collect()\n",
    "        #########################################\n",
    "        descriptor = [Chem.rdMolDescriptors.CalcAsphericity(alpha) for alpha in mols2]\n",
    "        evaluate_descriptor(fps, descriptor, 'Asphericity', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del descriptor\n",
    "        gc.collect()\n",
    "        descriptor = [Chem.rdMolDescriptors.CalcPBF(alpha) for alpha in mols2]\n",
    "        evaluate_descriptor(fps, descriptor, 'PBF', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del descriptor\n",
    "        gc.collect()\n",
    "        descriptor1 = [Chem.rdMolDescriptors.CalcPMI1(alpha) for alpha in mols2]\n",
    "        descriptor1 = Normalization(descriptor1)\n",
    "        evaluate_descriptor(fps, descriptor1, 'PMI1', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor2 = [Chem.rdMolDescriptors.CalcPMI2(alpha) for alpha in mols2]    \n",
    "        descriptor2 = Normalization(descriptor2)\n",
    "        evaluate_descriptor(fps, descriptor2, 'PMI2', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor3 = [Chem.rdMolDescriptors.CalcPMI3(alpha) for alpha in mols2]\n",
    "        descriptor3 = Normalization(descriptor3)\n",
    "        evaluate_descriptor(fps, descriptor3, 'PMI3', y_true, r2_list, esti_time, cpu_calc)\n",
    "        d1  = np.asarray(descriptor1).astype('float')\n",
    "        d2  = np.asarray(descriptor2).astype('float')\n",
    "        d3  = np.asarray(descriptor3).astype('float')\n",
    "        del descriptor1,descriptor2,descriptor3\n",
    "        gc.collect()\n",
    "        evaluate_descriptor(fps, d1+d2+d3, 'PMI_ALL_sum', y_true, r2_list, esti_time, cpu_calc)\n",
    "        dataset = [d1,d2,d3]\n",
    "        evaluate_descriptor(fps, dataset, 'PMI_ALL_ind', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del dataset,d1,d2,d3\n",
    "        # #########################################\n",
    "        descriptor1 = [Chem.rdMolDescriptors.CalcNPR1(alpha) for alpha in mols2]\n",
    "        evaluate_descriptor(fps, descriptor1, 'NPR1', y_true, r2_list, esti_time, cpu_calc)\n",
    "        descriptor2 = [Chem.rdMolDescriptors.CalcNPR2(alpha) for alpha in mols2]\n",
    "        evaluate_descriptor(fps, descriptor2, 'NPR2', y_true, r2_list, esti_time, cpu_calc)\n",
    "        d1  = np.asarray(descriptor1).astype('float')\n",
    "        d2  = np.asarray(descriptor2).astype('float')\n",
    "        del descriptor1, descriptor2\n",
    "        gc.collect()\n",
    "        evaluate_descriptor(fps, d1+d2, 'NPR_ALL_sum', y_true, r2_list, esti_time, cpu_calc)\n",
    "        dataset = [d1,d2]\n",
    "        evaluate_descriptor(fps, dataset, 'NPR_ALL_ind', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del dataset, d1,d2\n",
    "        #########################################\n",
    "        descriptor = [Chem.rdMolDescriptors.CalcRadiusOfGyration(alpha) for alpha in mols2]\n",
    "        evaluate_descriptor(fps, descriptor, 'RadiusOfGyration', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del descriptor\n",
    "        gc.collect()\n",
    "        descriptor = [Chem.rdMolDescriptors.CalcInertialShapeFactor(alpha) for alpha in mols2]\n",
    "        evaluate_descriptor(fps, descriptor, 'InertialShapeFactor', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del descriptor\n",
    "        gc.collect()\n",
    "        descriptor = [Chem.rdMolDescriptors.CalcEccentricity(alpha) for alpha in mols2]\n",
    "        evaluate_descriptor(fps, descriptor, 'Eccentricity', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del descriptor\n",
    "        gc.collect()\n",
    "        descriptor = [Chem.rdMolDescriptors.CalcSpherocityIndex(alpha) for alpha in mols2]\n",
    "        evaluate_descriptor(fps, descriptor, 'SpherocityIndex', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del descriptor\n",
    "        gc.collect()\n",
    "        descriptor = [Chem.rdMolDescriptors.MQNs_(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor, 'MQNs', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del descriptor\n",
    "        gc.collect()\n",
    "        descriptor = [Chem.rdMolDescriptors.CalcAUTOCORR2D(alpha) for alpha in mols]\n",
    "        evaluate_descriptor(fps, descriptor, 'AUTOCORR2D', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del descriptor\n",
    "        gc.collect()\n",
    "        descriptor = [Chem.rdMolDescriptors.CalcAUTOCORR3D(mols) for mols in mols2]\n",
    "        evaluate_descriptor(fps, descriptor, 'AUTOCORR3D', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del descriptor\n",
    "        gc.collect()\n",
    "        descriptor = [Chem.rdMolDescriptors.CalcRDF(mols) for mols in mols2]\n",
    "        descriptor = Normalization(descriptor)\n",
    "        evaluate_descriptor(fps, descriptor, 'RDF', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del descriptor\n",
    "        gc.collect()\n",
    "        descriptor = compute_descriptors_parallel(mols)\n",
    "        evaluate_descriptor(fps, descriptor, 'BCUT2D', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del descriptor\n",
    "        gc.collect()\n",
    "        descriptor = [Chem.rdMolDescriptors.CalcMORSE(mols) for mols in mols2]\n",
    "        descriptor = Normalization(descriptor)\n",
    "        evaluate_descriptor(fps, descriptor, 'MORSE', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del descriptor\n",
    "        gc.collect()\n",
    "        descriptor = [Chem.rdMolDescriptors.CalcWHIM(mols) for mols in mols2]\n",
    "        descriptor = Normalization(descriptor)\n",
    "        evaluate_descriptor(fps, descriptor, 'WHIM', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del descriptor\n",
    "        gc.collect()\n",
    "        descriptor = [Chem.rdMolDescriptors.CalcGETAWAY(mols) for mols in mols2]\n",
    "        descriptor = Normalization(descriptor)\n",
    "        evaluate_descriptor(fps, descriptor, 'GETAWAY', y_true, r2_list, esti_time, cpu_calc)\n",
    "        del descriptor\n",
    "        gc.collect()\n",
    "        if read_logs:\n",
    "            print(f\"-------Complete-------\")\n",
    "        #########################################\n",
    "    except Exception as e:\n",
    "        print(f\"Error in descriptors_list: {e}\")\n",
    "    finally:\n",
    "        print(r2_list)\n",
    "        # Ensure the target directory exists\n",
    "        def ensure_directory_exists(directory):\n",
    "            if not os.path.exists(directory):\n",
    "                try:\n",
    "                    os.makedirs(directory, exist_ok=True)  # Create directory if it doesn't exist\n",
    "                except Exception as e:\n",
    "                    print(f\"Error creating directory {directory}: {e}\")\n",
    "                    return False\n",
    "            return True\n",
    "\n",
    "        # Check if target_path is valid and directory exists\n",
    "        if ensure_directory_exists(target_path):\n",
    "            try:\n",
    "                # Save R2 scores\n",
    "                df = pd.DataFrame(list(r2_list.items()), columns=['descriptor', 'R2_score'])\n",
    "                df.to_csv(f\"{target_path}/{target_name}_fps_descriptor_individual_learning_results.csv\", index=False)\n",
    "\n",
    "                # Save estimation time if available\n",
    "                if esti_time:\n",
    "                    df2 = pd.DataFrame(list(esti_time.items()), columns=['descriptor', 'time_estimation'])\n",
    "                    df2.to_csv(f\"{target_path}/{target_name}_fps_descriptor_individual_learning_results_time.csv\", index=False)\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving CSV: {e}\")\n",
    "        else:\n",
    "            print(f\"Failed to ensure the directory {target_path} exists.\")\n",
    "        # cleanup_resources(locals())        \n",
    "    return r2_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_maker(fps2, name, dataset_name):\n",
    "    os.makedirs(target_path, exist_ok=True)\n",
    "    record_fps_tmp2 = pd.DataFrame({'Descriptor_Name': fps2.keys(), 'R2_Score': fps2.values()})\n",
    "    record_fps_tmp2 = record_fps_tmp2[~record_fps_tmp2['Descriptor_Name'].isin(['Original_tmp1', 'Original_tmp2'])]\n",
    "    record_fps_tmp2.to_csv(f\"{target_path}/record_fps2_r2_score_{name}.csv\", index=False)\n",
    "\n",
    "    original_r2 = record_fps_tmp2.loc[record_fps_tmp2['Descriptor_Name'] == 'Original', 'R2_Score'].values[0]\n",
    "\n",
    "    colors = []\n",
    "    for score in record_fps_tmp2['R2_Score']:\n",
    "        if score < 0:\n",
    "            colors.append('#ff9999')  # Negative R2 Score\n",
    "        elif score > original_r2:\n",
    "            colors.append('#66b3ff')  # Improved\n",
    "        else:\n",
    "            colors.append('#ffcc99')  # Not Improved\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(25, 7))\n",
    "    bars = ax.bar(record_fps_tmp2['Descriptor_Name'], record_fps_tmp2['R2_Score'], color=colors, edgecolor='black', linewidth=1)\n",
    "    plt.axhline(original_r2, color='red', linestyle='--', linewidth=2)\n",
    "    ax.set_facecolor('#f7f7f7')\n",
    "\n",
    "    plt.text(1.01, original_r2, 'Original', color='red', fontsize=12, ha='left', va='center', weight='bold', transform=ax.get_yaxis_transform())\n",
    "\n",
    "    plt.title(f\"The Improvement of additional Descriptors[{dataset_name}]\", fontsize=18, weight='bold')\n",
    "    plt.xlabel(\"Descriptor Name\", fontsize=14)\n",
    "    plt.ylabel(\"R2 Score\", fontsize=14)\n",
    "\n",
    "    for bar in bars:\n",
    "        ax.annotate(f\"{bar.get_height():.3f}\", \n",
    "                    (bar.get_x() + bar.get_width() / 2, - 0.05),\n",
    "                    ha='center', va='top', fontsize=10, rotation=90, color='black')\n",
    "\n",
    "    # for bar in bars:\n",
    "    #     ax.annotate(f\"{bar.get_height():.3f}\", \n",
    "    #                 (bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.03),\n",
    "    #                 ha='center', va='bottom', fontsize=10, rotation=90, color='black')\n",
    "    #                 # ha='center', va='bottom', fontsize=10, rotation=90, color='black')\n",
    "\n",
    "    # Adding legend for color representation\n",
    "    improved_patch = mpatches.Patch(color='#66b3ff', label='R2 Improved')\n",
    "    not_improved_patch = mpatches.Patch(color='#ffcc99', label='R2 Not Improved')\n",
    "    ax.legend(handles=[improved_patch, not_improved_patch], loc='lower right', fontsize=12)\n",
    "\n",
    "    plt.ylim(-0.5, 1.0)\n",
    "    plt.xticks(rotation=90, ha='right', fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', linewidth=0.7, alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{target_path}/r2_score_all_minus_inc_{name}.png\", dpi=300)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    print(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_to_numpy(*dataframes):\n",
    "    numpy_arrays = [df.to_numpy() if isinstance(df, pd.DataFrame) else df for df in dataframes]\n",
    "    if not all(isinstance(arr, np.ndarray) for arr in numpy_arrays):\n",
    "        raise ValueError(\"All inputs must be either pandas DataFrame or numpy array\")\n",
    "    return np.concatenate(numpy_arrays, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    group_nws = concatenate_to_numpy(x_ws, MACCS_ws, AvalonFP_ws)\n",
    "    group_nde = concatenate_to_numpy(x_de, MACCS_de, AvalonFP_de)\n",
    "    group_nlo = concatenate_to_numpy(x_lo, MACCS_lo, AvalonFP_lo)\n",
    "    group_nhu = concatenate_to_numpy(x_hu, MACCS_hu, AvalonFP_hu)\n",
    "    del x_ws, MACCS_ws, AvalonFP_ws\n",
    "    del x_de, MACCS_de, AvalonFP_de\n",
    "    del x_lo, MACCS_lo, AvalonFP_lo\n",
    "    del x_hu, MACCS_hu, AvalonFP_hu\n",
    "    gc.collect()\n",
    "except Exception as e:\n",
    "    print(f\"Error occured: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "makenew = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f\"{target_path}/dnn_feature_r2_score[ws].csv\"\n",
    "try:\n",
    "    if os.path.exists(file_path) and makenew==False:\n",
    "        pd_ws = pd.read_csv(file_path)\n",
    "        fps_learning_ws = pd_ws.set_index('descriptor_name')['R2score'].to_dict()\n",
    "    else:\n",
    "        fps_learning_ws = descriptors_list(group_nws, mol_ws, y_ws, 'ws496', target_path)\n",
    "        pd_ws = pd.DataFrame(list(fps_learning_ws.items()), columns=['descriptor_name', 'R2score'])\n",
    "        pd_ws.to_csv(file_path)\n",
    "except Exception as e:\n",
    "    print(f\"Error : {e}\")    \n",
    "plot_maker(fps_learning_ws, f'ws_{BATCHSIZE}batch_{EPOCHS}epoch_{lr}lr', 'ws496')\n",
    "#57m 13.4s <- 100 epochs\n",
    "#48m 36.1s\n",
    "#46m 2.9s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f\"{target_path}/dnn_feature_r2_score[de].csv\"\n",
    "try:\n",
    "    if os.path.exists(file_path) and makenew==False:\n",
    "        pd_de = pd.read_csv(file_path)\n",
    "        fps_learning_de = pd_de.set_index('descriptor_name')['R2score'].to_dict()\n",
    "    else:\n",
    "        fps_learning_de = descriptors_list(group_nde, mol_de, y_de, 'delaney', target_path)\n",
    "        pd_de = pd.DataFrame(list(fps_learning_de.items()), columns=['descriptor_name', 'R2score'])\n",
    "        pd_de.to_csv(f\"{target_path}/dnn_feature_r2_score[de].csv\")\n",
    "except Exception as e:\n",
    "    print(f\"Error : {e}\")    \n",
    "plot_maker(fps_learning_de, f'de_{BATCHSIZE}batch_{EPOCHS}epoch_{lr}lr', 'delaney')\n",
    "#75m 47.4s\n",
    "#50m 32.0s\n",
    "#50m 10.6s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f\"{target_path}/dnn_feature_r2_score[lo].csv\"\n",
    "try:\n",
    "    if os.path.exists(file_path) and makenew==False:\n",
    "        pd_lo = pd.read_csv(file_path)\n",
    "        fps_learning_lo = pd_lo.set_index('descriptor_name')['R2score'].to_dict()\n",
    "    else:\n",
    "        fps_learning_lo = descriptors_list(group_nlo, mol_lo, y_lo, 'lovric', target_path)\n",
    "        pd_lo = pd.DataFrame(list(fps_learning_lo.items()), columns=['descriptor_name', 'R2score'])\n",
    "        pd_lo.to_csv(f\"{target_path}/dnn_feature_r2_score[lo].csv\")\n",
    "except Exception as e:\n",
    "    print(f\"Error : {e}\")    \n",
    "plot_maker(fps_learning_lo, f'lo_{BATCHSIZE}batch_{EPOCHS}epoch_{lr}lr', 'lovric')\n",
    "## 77m 31.7s\n",
    "#50m 1.0s\n",
    "#50m 1.0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f\"{target_path}/dnn_feature_r2_score[hu].csv\"\n",
    "try:\n",
    "    if os.path.exists(file_path) and makenew==False:\n",
    "        pd_hu = pd.read_csv(file_path)\n",
    "        fps_learning_hu = pd_hu.set_index('descriptor_name')['R2score'].to_dict()\n",
    "    else:\n",
    "        fps_learning_hu = descriptors_list(group_nhu, mol_hu, y_hu, 'hussk', target_path)\n",
    "        pd_hu = pd.DataFrame(list(fps_learning_hu.items()), columns=['descriptor_name', 'R2score'])\n",
    "        pd_hu.to_csv(f\"{target_path}/dnn_feature_r2_score[hu].csv\")\n",
    "except Exception as e:\n",
    "    print(f\"Error : {e}\")    \n",
    "plot_maker(fps_learning_hu, f'hu_{BATCHSIZE}batch_{EPOCHS}epoch_{lr}lr', 'hussk')\n",
    "#83m 13.5s\n",
    "#56m 35.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = True\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f\"{target_path}/dnn_feature_r2_score[ws].csv\"\n",
    "try:\n",
    "    if os.path.exists(file_path) and makenew==False:\n",
    "        pd_ws = pd.read_csv(file_path)\n",
    "        fps_learning_ws = pd_ws.set_index('descriptor_name')['R2score'].to_dict()\n",
    "    else:\n",
    "        fps_learning_ws = descriptors_list(group_nws, mol_ws, y_ws, 'ws496', target_path, cpu_calc=True)\n",
    "        pd_ws = pd.DataFrame(list(fps_learning_ws.items()), columns=['descriptor_name', 'R2score'])\n",
    "        pd_ws.to_csv(file_path)\n",
    "except Exception as e:\n",
    "    print(f\"Error : {e}\")    \n",
    "plot_maker(fps_learning_ws, f'ws_{BATCHSIZE}batch_{EPOCHS}epoch_{lr}lr', 'ws496')\n",
    "#57m 13.4s <- 100 epochs\n",
    "#48m 36.1s\n",
    "#46m 2.9s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f\"{target_path}/dnn_feature_r2_score[de].csv\"\n",
    "try:\n",
    "    if os.path.exists(file_path) and makenew==False:\n",
    "        pd_de = pd.read_csv(file_path)\n",
    "        fps_learning_de = pd_de.set_index('descriptor_name')['R2score'].to_dict()\n",
    "    else:\n",
    "        fps_learning_de = descriptors_list(group_nde, mol_de, y_de, 'delaney', target_path, cpu_calc=True)\n",
    "        pd_de = pd.DataFrame(list(fps_learning_de.items()), columns=['descriptor_name', 'R2score'])\n",
    "        pd_de.to_csv(f\"{target_path}/dnn_feature_r2_score[de].csv\")\n",
    "except Exception as e:\n",
    "    print(f\"Error : {e}\")    \n",
    "plot_maker(fps_learning_de, f'de_{BATCHSIZE}batch_{EPOCHS}epoch_{lr}lr', 'delaney')\n",
    "#75m 47.4s\n",
    "#50m 32.0s\n",
    "#50m 10.6s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f\"{target_path}/dnn_feature_r2_score[lo].csv\"\n",
    "try:\n",
    "    if os.path.exists(file_path) and makenew==False:\n",
    "        pd_lo = pd.read_csv(file_path)\n",
    "        fps_learning_lo = pd_lo.set_index('descriptor_name')['R2score'].to_dict()\n",
    "    else:\n",
    "        fps_learning_lo = descriptors_list(group_nlo, mol_lo, y_lo, 'lovric', target_path, cpu_calc=True)\n",
    "        pd_lo = pd.DataFrame(list(fps_learning_lo.items()), columns=['descriptor_name', 'R2score'])\n",
    "        pd_lo.to_csv(f\"{target_path}/dnn_feature_r2_score[lo].csv\")\n",
    "except Exception as e:\n",
    "    print(f\"Error : {e}\")    \n",
    "plot_maker(fps_learning_lo, f'lo_{BATCHSIZE}batch_{EPOCHS}epoch_{lr}lr', 'lovric')\n",
    "## 77m 31.7s\n",
    "#50m 1.0s\n",
    "#50m 1.0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f\"{target_path}/dnn_feature_r2_score[hu].csv\"\n",
    "try:\n",
    "    if os.path.exists(file_path) and makenew==False:\n",
    "        pd_hu = pd.read_csv(file_path)\n",
    "        fps_learning_hu = pd_hu.set_index('descriptor_name')['R2score'].to_dict()\n",
    "    else:\n",
    "        fps_learning_hu = descriptors_list(group_nhu, mol_hu, y_hu, 'hussk', target_path, cpu_calc=True)\n",
    "        pd_hu = pd.DataFrame(list(fps_learning_hu.items()), columns=['descriptor_name', 'R2score'])\n",
    "        pd_hu.to_csv(f\"{target_path}/dnn_feature_r2_score[hu].csv\")\n",
    "except Exception as e:\n",
    "    print(f\"Error : {e}\")    \n",
    "plot_maker(fps_learning_hu, f'hu_{BATCHSIZE}batch_{EPOCHS}epoch_{lr}lr', 'hussk')\n",
    "#83m 13.5s\n",
    "#56m 35.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_r2_score(data, dataset_name, target_path):\n",
    "    ax = data.plot.bar(x='descriptor_name', y='R2score', figsize=(30, 10), ylabel='R2score', ylim=(-0.5, 1))\n",
    "    plt.axhline(data.loc[0][1], color='red', linestyle='--', linewidth=3)\n",
    "    plt.title(f\"The prediction improvement with additional chemical descriptors [{dataset_name}]\", fontsize=20)\n",
    "    \n",
    "    for index, value in enumerate(data['R2score']):\n",
    "        if value <= -0.5:\n",
    "            ax.text(index, value - 0.05, f'{value:.2f}', ha='center', va='bottom', fontsize=12, color='black')\n",
    "        else:\n",
    "            ax.text(index, value + 0.01, f'{value:.2f}', ha='center', va='bottom', fontsize=12, color='black')\n",
    "\n",
    "    plt.savefig(f\"{target_path}/r2_score_each_descriptors_{dataset_name}.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "plot_r2_score(pd_ws, 'ws', target_path)\n",
    "plot_r2_score(pd_de, 'de', target_path)\n",
    "plot_r2_score(pd_lo, 'lo', target_path)\n",
    "plot_r2_score(pd_hu, 'hu', target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_ws = pd_ws.loc[pd_ws['R2score'] > pd_ws.loc[0][1]]\n",
    "res_de = pd_de.loc[pd_de['R2score'] > pd_de.loc[0][1]]\n",
    "res_lo = pd_lo.loc[pd_lo['R2score'] > pd_lo.loc[0][1]]\n",
    "res_hu = pd_hu.loc[pd_hu['R2score'] > pd_hu.loc[0][1]]\n",
    "\n",
    "re_arrange_ws = res_ws.sort_values(by='R2score',ascending=False)\n",
    "re_arrange_de = res_de.sort_values(by='R2score',ascending=False)\n",
    "re_arrange_lo = res_lo.sort_values(by='R2score',ascending=False)\n",
    "re_arrange_hu = res_hu.sort_values(by='R2score',ascending=False)\n",
    "\n",
    "re_arrange_ws.to_csv(f\"{target_path}/[3]_individual_r2_score_higher_than_original_prediction[ws].csv\", index=False)\n",
    "re_arrange_de.to_csv(f\"{target_path}/[3]_individual_r2_score_higher_than_original_prediction[de].csv\", index=False)\n",
    "re_arrange_lo.to_csv(f\"{target_path}/[3]_individual_r2_score_higher_than_original_prediction[lo].csv\", index=False)\n",
    "re_arrange_hu.to_csv(f\"{target_path}/[3]_individual_r2_score_higher_than_original_prediction[hu].csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_ws.max(), pd_de.max(), pd_lo.max(), pd_hu.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = [pd_ws,pd_de.iloc[:,1],pd_lo.iloc[:,1],pd_hu.iloc[:,1]]\n",
    "res = pd.concat(res, axis=1)\n",
    "res.columns = ['descriptor_name', 'ws496', 'delaney', 'lovrics', 'huusk']\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = res.plot(x='descriptor_name', y=['ws496', 'delaney', 'lovrics', 'huusk'], figsize=(35, 15), stacked=True, kind='bar', ylim=(-0.5, None))\n",
    "plt.legend(loc=1, fontsize=12)\n",
    "\n",
    "# 레이블 추가\n",
    "for bar in ax.patches:\n",
    "    height = bar.get_height()\n",
    "    width = bar.get_width()\n",
    "    x = bar.get_x()\n",
    "    y = bar.get_y()\n",
    "    label_text = f'{height:0.4f}'\n",
    "    label_x = x + width / 2\n",
    "    label_y = y + height / 2\n",
    "\n",
    "    if height > 0 or height < -0.5:\n",
    "        ax.text(label_x, label_y, label_text, ha='center', size=9, va='center', rotation='vertical', fontweight='bold')\n",
    "\n",
    "# 축 및 제목 설정\n",
    "plt.xlabel(\"Descriptor Name\", fontsize=14)\n",
    "plt.ylabel(\"R2 Score\", fontsize=14)\n",
    "plt.title(\"DNN Improvement with Additional Chemical Descriptors\", fontsize=18, fontweight='bold')\n",
    "\n",
    "# 파일 저장 및 그래프 출력\n",
    "plt.savefig(f\"{target_path}/chem_descriptor_r2score_result_ALL_vertical.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_print_count = 20\n",
    "selected_descriptors = ['Original','ExactMolWt','MolWt', 'MolLogP', 'MolMR']\n",
    "top_descriptors = res.nlargest(target_print_count-len(selected_descriptors), 'delaney')  \n",
    "\n",
    "filtered_res = pd.concat([res[res['descriptor_name'].isin(selected_descriptors)], top_descriptors])\n",
    "filtered_res = filtered_res.drop_duplicates(subset='descriptor_name', keep='first')\n",
    "\n",
    "best_dataset = 'delaney'\n",
    "\n",
    "colors = ['#0000ff', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "font = {'size': 18, 'weight': 'bold'}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 10))\n",
    "\n",
    "bar_width = 0.2\n",
    "index = np.arange(len(filtered_res['descriptor_name']))\n",
    "\n",
    "max_scores = filtered_res[['ws496', 'delaney', 'lovrics', 'huusk']].max(axis=1)\n",
    "\n",
    "for i in range(len(filtered_res)):\n",
    "    # ws496\n",
    "    alpha_ws496 = 1.0 if filtered_res['ws496'].iloc[i] == max_scores.iloc[i] else 0.3\n",
    "    ax.bar(index[i] - 1.5 * bar_width, filtered_res['ws496'].iloc[i], bar_width, label='ws496' if i == 0 else \"\", \n",
    "           color=colors[0], alpha=alpha_ws496)\n",
    "    \n",
    "    # delaney\n",
    "    alpha_delaney = 1.0 if filtered_res['delaney'].iloc[i] == max_scores.iloc[i] else 0.3\n",
    "    ax.bar(index[i] - 0.5 * bar_width, filtered_res['delaney'].iloc[i], bar_width, label='delaney' if i == 0 else \"\", \n",
    "           color=colors[1], alpha=alpha_delaney)\n",
    "    \n",
    "    # lovrics\n",
    "    alpha_lovrics = 1.0 if filtered_res['lovrics'].iloc[i] == max_scores.iloc[i] else 0.3\n",
    "    ax.bar(index[i] + 0.5 * bar_width, filtered_res['lovrics'].iloc[i], bar_width, label='lovrics' if i == 0 else \"\", \n",
    "           color=colors[2], alpha=alpha_lovrics)\n",
    "    \n",
    "    # huusk\n",
    "    alpha_huusk = 1.0 if filtered_res['huusk'].iloc[i] == max_scores.iloc[i] else 0.3\n",
    "    ax.bar(index[i] + 1.5 * bar_width, filtered_res['huusk'].iloc[i], bar_width, label='huusk' if i == 0 else \"\", \n",
    "           color=colors[3], alpha=alpha_huusk)\n",
    "\n",
    "for i in range(len(filtered_res)):\n",
    "    ax.text(i - 1.5 * bar_width, -0.1, f'{filtered_res[\"ws496\"].iloc[i]:.4f}', \n",
    "            ha='center', va='top', fontsize=12, fontweight='bold', rotation=90)\n",
    "    ax.text(i - 0.5 * bar_width, -0.1, f'{filtered_res[\"delaney\"].iloc[i]:.4f}', \n",
    "            ha='center', va='top', fontsize=12, fontweight='bold', rotation=90)\n",
    "    ax.text(i + 0.5 * bar_width, -0.1, f'{filtered_res[\"lovrics\"].iloc[i]:.4f}', \n",
    "            ha='center', va='top', fontsize=12, fontweight='bold', rotation=90)\n",
    "    ax.text(i + 1.5 * bar_width, -0.1, f'{filtered_res[\"huusk\"].iloc[i]:.4f}', \n",
    "            ha='center', va='top', fontsize=12, fontweight='bold', rotation=90)\n",
    "\n",
    "ax.set_xlabel('Descriptor Name', fontsize=18, labelpad=10)\n",
    "ax.set_ylabel('R2 Score', fontsize=18, labelpad=10)\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(filtered_res['descriptor_name'], rotation=45, ha='right', fontsize=12)\n",
    "\n",
    "ax.set_ylim(-0.5, 1.0)\n",
    "\n",
    "ax.legend(loc='upper left', fontsize=14, title='Datasets', title_fontsize=14)\n",
    "\n",
    "ax.grid(True, which='major', axis='y', linestyle='--', linewidth=0.5, color='gray', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{target_path}/chem_descriptor_r2score.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "font = {'size': 20, 'weight': 'bold'}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "ax = res.plot(x='descriptor_name', y=['ws496', 'delaney', 'lovrics', 'huusk'], figsize=(30, 42), stacked=True, kind='barh', xlim=(-0.5, None))\n",
    "\n",
    "ax.invert_yaxis()\n",
    "\n",
    "plt.legend(loc=1, fontsize=25)\n",
    "\n",
    "for p in ax.patches:\n",
    "    h, w, x, y = p.get_height(), p.get_width(), p.get_x(), p.get_y()\n",
    "    text = f'{w:0.4f}'\n",
    "\n",
    "\n",
    "    if w > 0 or w < -0.5:\n",
    "        ax.annotate(text=text, xy=(x + w / 2, y + h / 2), ha='center', va='center', size=18, fontweight='bold')\n",
    "\n",
    "plt.ylabel(\"Chemical Descriptors\", fontsize=25, fontweight='bold')\n",
    "plt.xlabel(\"R2 Score\", fontsize=25, fontweight='bold')\n",
    "plt.title(\"DNN Improvement with Additional Chemical Descriptors\", fontsize=30, fontweight='bold', pad=20)\n",
    "\n",
    "plt.savefig(f\"{target_path}/chem_descriptor_r2score_result_ALL_horizontal.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_print_count = 20\n",
    "selected_descriptors = ['Original', 'ExactMolWt', 'MolWt', 'MolLogP', 'MolMR']\n",
    "top_descriptors = res.nlargest(target_print_count - len(selected_descriptors), 'delaney')\n",
    "\n",
    "filtered_res = pd.concat([res[res['descriptor_name'].isin(selected_descriptors)], top_descriptors])\n",
    "filtered_res = filtered_res.drop_duplicates(subset='descriptor_name', keep='first')\n",
    "\n",
    "best_dataset = 'delaney'\n",
    "\n",
    "colors = ['#0000ff', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "font = {'size': 18, 'weight': 'bold'}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 18))\n",
    "\n",
    "bar_width = 0.2\n",
    "index = np.arange(len(filtered_res['descriptor_name']))\n",
    "\n",
    "max_scores = filtered_res[['ws496', 'delaney', 'lovrics', 'huusk']].max(axis=1)\n",
    "\n",
    "for i in range(len(filtered_res)):\n",
    "    alpha_ws496 = 1.0 if filtered_res['ws496'].iloc[i] == max_scores.iloc[i] else 0.3\n",
    "    ax.barh(index[i] - 1.5 * bar_width, filtered_res['ws496'].iloc[i], bar_width, label='ws496' if i == 0 else \"\", \n",
    "            color=colors[0], alpha=alpha_ws496)\n",
    "\n",
    "    alpha_delaney = 1.0 if filtered_res['delaney'].iloc[i] == max_scores.iloc[i] else 0.3\n",
    "    ax.barh(index[i] - 0.5 * bar_width, filtered_res['delaney'].iloc[i], bar_width, label='delaney' if i == 0 else \"\", \n",
    "            color=colors[1], alpha=alpha_delaney)\n",
    "\n",
    "    alpha_lovrics = 1.0 if filtered_res['lovrics'].iloc[i] == max_scores.iloc[i] else 0.3\n",
    "    ax.barh(index[i] + 0.5 * bar_width, filtered_res['lovrics'].iloc[i], bar_width, label='lovrics' if i == 0 else \"\", \n",
    "            color=colors[2], alpha=alpha_lovrics)\n",
    "\n",
    "    alpha_huusk = 1.0 if filtered_res['huusk'].iloc[i] == max_scores.iloc[i] else 0.3\n",
    "    ax.barh(index[i] + 1.5 * bar_width, filtered_res['huusk'].iloc[i], bar_width, label='huusk' if i == 0 else \"\", \n",
    "            color=colors[3], alpha=alpha_huusk)\n",
    "\n",
    "for i in range(len(filtered_res)):\n",
    "    ax.text(-0.1, i - 1.5 * bar_width, f'{filtered_res[\"ws496\"].iloc[i]:.4f}', \n",
    "            ha='right', va='center', fontsize=12, fontweight='bold')\n",
    "    ax.text(-0.1, i - 0.5 * bar_width, f'{filtered_res[\"delaney\"].iloc[i]:.4f}', \n",
    "            ha='right', va='center', fontsize=12, fontweight='bold')\n",
    "    ax.text(-0.1, i + 0.5 * bar_width, f'{filtered_res[\"lovrics\"].iloc[i]:.4f}', \n",
    "            ha='right', va='center', fontsize=12, fontweight='bold')\n",
    "    ax.text(-0.1, i + 1.5 * bar_width, f'{filtered_res[\"huusk\"].iloc[i]:.4f}', \n",
    "            ha='right', va='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Descriptor Name', fontsize=18, labelpad=10)\n",
    "ax.set_xlabel('R2 Score', fontsize=18, labelpad=10)\n",
    "ax.set_yticks(index)\n",
    "ax.set_yticklabels(filtered_res['descriptor_name'][::-1], rotation=0, ha='right', fontsize=12)\n",
    "\n",
    "ax.set_xlim(-0.5, 1.0)\n",
    "\n",
    "ax.legend(loc='upper right', fontsize=14, title='Datasets', title_fontsize=14)\n",
    "\n",
    "ax.grid(True, which='major', axis='x', linestyle='--', linewidth=0.5, color='gray', alpha=0.7)\n",
    "\n",
    "plt.title('Comparison of Chemical Descriptors by R2 Score', fontsize=20, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{target_path}/chem_descriptor_r2score_horizontal_title.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_r2_scores(res, target_print_count=20, selected_descriptors=['Original', 'ExactMolWt', 'MolWt', 'MolLogP', 'MolMR'], orientation='vertical', target_path=\".\", title='Comparison of Chemical Descriptors by DNN'):\n",
    "    top_descriptors = res.nlargest(target_print_count - len(selected_descriptors), 'delaney')\n",
    "    filtered_res = pd.concat([res[res['descriptor_name'].isin(selected_descriptors)], top_descriptors])\n",
    "    filtered_res = filtered_res.drop_duplicates(subset='descriptor_name', keep='first')\n",
    "\n",
    "    best_dataset = 'delaney'\n",
    "    colors = ['#0000ff', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "    font = {'size': 18, 'weight': 'bold'}\n",
    "    plt.rc('font', **font)\n",
    "\n",
    "    if orientation == 'vertical':\n",
    "        fig, ax = plt.subplots(figsize=(18, 10))\n",
    "        bar_width = 0.2\n",
    "        index = np.arange(len(filtered_res['descriptor_name']))\n",
    "\n",
    "        max_scores = filtered_res[['ws496', 'delaney', 'lovrics', 'huusk']].max(axis=1)\n",
    "\n",
    "        for i in range(len(filtered_res)):\n",
    "            alpha_ws496 = 1.0 if filtered_res['ws496'].iloc[i] == max_scores.iloc[i] else 0.3\n",
    "            ax.bar(index[i] - 1.5 * bar_width, filtered_res['ws496'].iloc[i], bar_width, label='ws496' if i == 0 else \"\", \n",
    "                   color=colors[0], alpha=alpha_ws496)\n",
    "\n",
    "            alpha_delaney = 1.0 if filtered_res['delaney'].iloc[i] == max_scores.iloc[i] else 0.3\n",
    "            ax.bar(index[i] - 0.5 * bar_width, filtered_res['delaney'].iloc[i], bar_width, label='delaney' if i == 0 else \"\", \n",
    "                   color=colors[1], alpha=alpha_delaney)\n",
    "\n",
    "            alpha_lovrics = 1.0 if filtered_res['lovrics'].iloc[i] == max_scores.iloc[i] else 0.3\n",
    "            ax.bar(index[i] + 0.5 * bar_width, filtered_res['lovrics'].iloc[i], bar_width, label='lovrics' if i == 0 else \"\", \n",
    "                   color=colors[2], alpha=alpha_lovrics)\n",
    "\n",
    "            alpha_huusk = 1.0 if filtered_res['huusk'].iloc[i] == max_scores.iloc[i] else 0.3\n",
    "            ax.bar(index[i] + 1.5 * bar_width, filtered_res['huusk'].iloc[i], bar_width, label='huusk' if i == 0 else \"\", \n",
    "                   color=colors[3], alpha=alpha_huusk)\n",
    "\n",
    "        for i in range(len(filtered_res)):\n",
    "            if filtered_res['ws496'].iloc[i] == max_scores.iloc[i]:\n",
    "                ax.text(i - 1.5 * bar_width, -0.1, f'*{filtered_res[\"ws496\"].iloc[i]:.4f}', \n",
    "                        ha='center', va='top', fontsize=12, fontweight='bold', rotation=90)\n",
    "            else:\n",
    "                ax.text(i - 1.5 * bar_width, -0.1, f'{filtered_res[\"ws496\"].iloc[i]:.4f}', \n",
    "                        ha='center', va='top', fontsize=12, rotation=90)\n",
    "\n",
    "            if filtered_res['delaney'].iloc[i] == max_scores.iloc[i]:\n",
    "                ax.text(i - 0.5 * bar_width, -0.1, f'*{filtered_res[\"delaney\"].iloc[i]:.4f}', \n",
    "                        ha='center', va='top', fontsize=12, fontweight='bold', rotation=90)\n",
    "            else:\n",
    "                ax.text(i - 0.5 * bar_width, -0.1, f'{filtered_res[\"delaney\"].iloc[i]:.4f}', \n",
    "                        ha='center', va='top', fontsize=12, rotation=90)\n",
    "\n",
    "            if filtered_res['lovrics'].iloc[i] == max_scores.iloc[i]:\n",
    "                ax.text(i + 0.5 * bar_width, -0.1, f'*{filtered_res[\"lovrics\"].iloc[i]:.4f}', \n",
    "                        ha='center', va='top', fontsize=12, fontweight='bold', rotation=90)\n",
    "            else:\n",
    "                ax.text(i + 0.5 * bar_width, -0.1, f'{filtered_res[\"lovrics\"].iloc[i]:.4f}', \n",
    "                        ha='center', va='top', fontsize=12, rotation=90)\n",
    "\n",
    "            if filtered_res['huusk'].iloc[i] == max_scores.iloc[i]:\n",
    "                ax.text(i + 1.5 * bar_width, -0.1, f'*{filtered_res[\"huusk\"].iloc[i]:.4f}', \n",
    "                        ha='center', va='top', fontsize=12, fontweight='bold', rotation=90)\n",
    "            else:\n",
    "                ax.text(i + 1.5 * bar_width, -0.1, f'{filtered_res[\"huusk\"].iloc[i]:.4f}', \n",
    "                        ha='center', va='top', fontsize=12, rotation=90)\n",
    "\n",
    "        ax.set_xlabel('Descriptor Name', fontsize=18, labelpad=10)\n",
    "        ax.set_ylabel('R2 Score', fontsize=18, labelpad=10)\n",
    "        ax.set_xticks(index)\n",
    "        ax.set_xticklabels(filtered_res['descriptor_name'], rotation=45, ha='right', fontsize=12)\n",
    "        plt.figtext(0.75, 0.23, \"* indicates the best score in the dataset\", ha=\"center\", fontsize=12, color='gray')\n",
    "\n",
    "    else:\n",
    "        fig, ax = plt.subplots(figsize=(10, 18))\n",
    "        bar_width = 0.2\n",
    "        index = np.arange(len(filtered_res['descriptor_name']))\n",
    "\n",
    "        max_scores = filtered_res[['ws496', 'delaney', 'lovrics', 'huusk']].max(axis=1)\n",
    "\n",
    "        for i in range(len(filtered_res)):\n",
    "            alpha_ws496 = 1.0 if filtered_res['ws496'].iloc[i] == max_scores.iloc[i] else 0.3\n",
    "            ax.barh(index[i] - 1.5 * bar_width, filtered_res['ws496'].iloc[i], bar_width, label='ws496' if i == 0 else \"\", \n",
    "                    color=colors[0], alpha=alpha_ws496)\n",
    "\n",
    "            alpha_delaney = 1.0 if filtered_res['delaney'].iloc[i] == max_scores.iloc[i] else 0.3\n",
    "            ax.barh(index[i] - 0.5 * bar_width, filtered_res['delaney'].iloc[i], bar_width, label='delaney' if i == 0 else \"\", \n",
    "                    color=colors[1], alpha=alpha_delaney)\n",
    "\n",
    "            alpha_lovrics = 1.0 if filtered_res['lovrics'].iloc[i] == max_scores.iloc[i] else 0.3\n",
    "            ax.barh(index[i] + 0.5 * bar_width, filtered_res['lovrics'].iloc[i], bar_width, label='lovrics' if i == 0 else \"\", \n",
    "                    color=colors[2], alpha=alpha_lovrics)\n",
    "\n",
    "            alpha_huusk = 1.0 if filtered_res['huusk'].iloc[i] == max_scores.iloc[i] else 0.3\n",
    "            ax.barh(index[i] + 1.5 * bar_width, filtered_res['huusk'].iloc[i], bar_width, label='huusk' if i == 0 else \"\", \n",
    "                    color=colors[3], alpha=alpha_huusk)\n",
    "\n",
    "        for i in range(len(filtered_res)):\n",
    "            if filtered_res['ws496'].iloc[i] == max_scores.iloc[i]:\n",
    "                ax.text(-0.1, i - 1.5 * bar_width, f'*{filtered_res[\"ws496\"].iloc[i]:.4f}', \n",
    "                        ha='right', va='center', fontsize=12, fontweight='bold')\n",
    "            else:\n",
    "                ax.text(-0.1, i - 1.5 * bar_width, f'{filtered_res[\"ws496\"].iloc[i]:.4f}', \n",
    "                        ha='right', va='center', fontsize=12)\n",
    "\n",
    "            if filtered_res['delaney'].iloc[i] == max_scores.iloc[i]:\n",
    "                ax.text(-0.1, i - 0.5 * bar_width, f'*{filtered_res[\"delaney\"].iloc[i]:.4f}', \n",
    "                        ha='right', va='center', fontsize=12, fontweight='bold')\n",
    "            else:\n",
    "                ax.text(-0.1, i - 0.5 * bar_width, f'{filtered_res[\"delaney\"].iloc[i]:.4f}', \n",
    "                        ha='right', va='center', fontsize=12)\n",
    "\n",
    "            if filtered_res['lovrics'].iloc[i] == max_scores.iloc[i]:\n",
    "                ax.text(-0.1, i + 0.5 * bar_width, f'*{filtered_res[\"lovrics\"].iloc[i]:.4f}', \n",
    "                        ha='right', va='center', fontsize=12, fontweight='bold')\n",
    "            else:\n",
    "                ax.text(-0.1, i + 0.5 * bar_width, f'{filtered_res[\"lovrics\"].iloc[i]:.4f}', \n",
    "                        ha='right', va='center', fontsize=12)\n",
    "\n",
    "            if filtered_res['huusk'].iloc[i] == max_scores.iloc[i]:\n",
    "                ax.text(-0.1, i + 1.5 * bar_width, f'*{filtered_res[\"huusk\"].iloc[i]:.4f}', \n",
    "                        ha='right', va='center', fontsize=12, fontweight='bold')\n",
    "            else:\n",
    "                ax.text(-0.1, i + 1.5 * bar_width, f'{filtered_res[\"huusk\"].iloc[i]:.4f}', \n",
    "                        ha='right', va='center', fontsize=12)\n",
    "\n",
    "        ax.set_ylabel('Descriptor Name', fontsize=18, labelpad=10)\n",
    "        ax.set_xlabel('R2 Score', fontsize=18, labelpad=10)\n",
    "        ax.set_yticks(index)\n",
    "        ax.set_yticklabels(filtered_res['descriptor_name'], rotation=0, ha='right', fontsize=12)\n",
    "        plt.figtext(0.47, 0.055, \"* indicates the best score in the dataset\", ha=\"center\", fontsize=12, color='gray')\n",
    "\n",
    "    ax.set_ylim(-0.5, 1.0) if orientation == 'vertical' else ax.set_xlim(-0.5, 1.0)\n",
    "\n",
    "    ax.legend(loc='upper left', fontsize=14, title='Datasets', title_fontsize=14, bbox_to_anchor=(1.05, 1))\n",
    "\n",
    "    ax.grid(True, which='major', axis='y' if orientation == 'vertical' else 'x', linestyle='--', linewidth=0.5, color='gray', alpha=0.7)\n",
    "\n",
    "    plt.title(title, fontsize=20, fontweight='bold', pad=20)\n",
    "\n",
    "    \n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{target_path}/DNN_descriptor_r2score_{orientation}.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_r2_scores(res, 20 ,orientation='vertical', target_path=target_path)\n",
    "plot_r2_scores(res, 20 ,orientation='horizontal', target_path=target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
